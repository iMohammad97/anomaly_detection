{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ==============================\n",
    "# 1) Imports and Metric Functions\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score, precision_score\n",
    "import glob, os, sys\n",
    "import kaleido\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def pointwise_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Timepoint-wise precision: fraction of detected anomalies that are correct.\n",
    "    \"\"\"\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "def make_event(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Converts binary sequences (y_true and y_pred) into a list of (start, end) event tuples.\n",
    "    \"\"\"\n",
    "    y_true_starts = np.argwhere(np.diff(y_true.flatten(), prepend=0) == 1).flatten()\n",
    "    y_true_ends   = np.argwhere(np.diff(y_true.flatten(), append=0) == -1).flatten()\n",
    "    y_true_events = list(zip(y_true_starts, y_true_ends))\n",
    "\n",
    "    y_pred_starts = np.argwhere(np.diff(y_pred, prepend=0) == 1).flatten()\n",
    "    y_pred_ends   = np.argwhere(np.diff(y_pred, append=0) == -1).flatten()\n",
    "    y_pred_events = list(zip(y_pred_starts, y_pred_ends))\n",
    "\n",
    "    return y_true_events, y_pred_events\n",
    "\n",
    "def event_wise_recall(y_true_events, y_pred_events):\n",
    "    \"\"\"\n",
    "    Event-based recall. We consider an event 'detected' if the predicted event\n",
    "    overlaps with the true event in any way.\n",
    "    \"\"\"\n",
    "    detected_events = 0\n",
    "    for true_event in y_true_events:\n",
    "        true_start, true_end = true_event\n",
    "        for pred_event in y_pred_events:\n",
    "            pred_start, pred_end = pred_event\n",
    "            if pred_end >= true_start and pred_start <= true_end:\n",
    "                detected_events += 1\n",
    "                break\n",
    "    return detected_events / len(y_true_events) if y_true_events else 0\n",
    "\n",
    "def composite_f_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Combines timepoint precision and event-wise recall into a single F-score.\n",
    "    \"\"\"\n",
    "    prt = pointwise_precision(y_true, y_pred)\n",
    "    y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "    rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "    if prt + rece == 0:\n",
    "        return 0\n",
    "    return 2 * (prt * rece) / (prt + rece)\n",
    "\n",
    "def custom_auc_with_perfect_point(y_true, anomaly_scores, threshold_steps=100, plot=False):\n",
    "    \"\"\"\n",
    "    Generates thresholds, computes precision (timepoint) and recall (event-wise)\n",
    "    pairs, checks for a perfect point, and computes the AUC (area under the curve)\n",
    "    on the PR plane.\n",
    "    \"\"\"\n",
    "    percentiles = np.linspace(np.min(anomaly_scores), np.max(anomaly_scores) + 1e-7, threshold_steps)\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    perfect_point_found = False\n",
    "\n",
    "    for threshold in percentiles:\n",
    "        y_pred = (anomaly_scores >= threshold).astype(int)\n",
    "        prt = pointwise_precision(y_true, y_pred)\n",
    "\n",
    "        y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "        rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "\n",
    "        precision_list.append(prt)\n",
    "        recall_list.append(rece)\n",
    "\n",
    "        if prt == 1 and rece == 1:\n",
    "            perfect_point_found = True\n",
    "            break\n",
    "\n",
    "    # Compute AUC (precision vs recall)\n",
    "    custom_area = auc(recall_list, precision_list)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_list, precision_list, marker='o', label=f\"AUC = {custom_area:.4f}\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    return custom_area, perfect_point_found\n",
    "\n",
    "def compute_auc_pr(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-PR for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, anomaly_scores)\n",
    "        auc_pr = auc(recall, precision)\n",
    "    except ValueError:\n",
    "        print(\"AUC-PR computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_pr = np.nan\n",
    "    return auc_pr\n",
    "\n",
    "def compute_auc_roc(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-ROC for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(y_true, anomaly_scores)\n",
    "    except ValueError:\n",
    "        print(\"AUC-ROC computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_roc = np.nan\n",
    "    return auc_roc\n",
    "\n",
    "# ========== Utils ==========\n",
    "def create_windows(data, window_size: int, step_size: int = 1):\n",
    "    \"\"\"\n",
    "    Given a 2D array data of shape (N, features), create overlapping windows\n",
    "    of shape (window_size, features). Returns array of shape (M, window_size, features).\n",
    "    If data is shorter than window_size, returns None.\n",
    "    \"\"\"\n",
    "    if data is None or data.shape[0] < window_size:\n",
    "        return None\n",
    "    windows = []\n",
    "    N = data.shape[0]\n",
    "    for i in range(0, N - window_size + 1, step_size):\n",
    "        window = data[i : i + window_size]\n",
    "        windows.append(window)\n",
    "    # Return as float32 to avoid dtype mismatch\n",
    "    return np.stack(windows, axis=0).astype(np.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2) Original LSTMAutoencoder Class\n",
    "# ==============================\n",
    "\n",
    "class LSTMAutoencoder:\n",
    "    def __init__(self,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 labels,\n",
    "                 timesteps: int = 128,\n",
    "                 features: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 lstm_units: int = 64,\n",
    "                 step_size: int = 1,\n",
    "                 threshold_sigma=2.0,\n",
    "                 seed: int = 0):\n",
    "\n",
    "        self.train_data = train_data.astype(np.float32) if train_data is not None else None\n",
    "        self.test_data = test_data.astype(np.float32) if test_data is not None else None\n",
    "        self.labels = labels.astype(int) if labels is not None else None\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.step_size = step_size\n",
    "        self.threshold_sigma = threshold_sigma\n",
    "\n",
    "        # Prepare windowed data\n",
    "        self.train_data_window = create_windows(self.train_data, timesteps, step_size)\n",
    "        self.test_data_window = create_windows(self.test_data, timesteps, 1)\n",
    "\n",
    "        # Model placeholders\n",
    "        self.model = None\n",
    "        self.threshold = 0\n",
    "\n",
    "        # Arrays to hold predictions\n",
    "        if self.test_data_window is not None:\n",
    "            self.predictions_windows = np.zeros(len(self.test_data_window))\n",
    "        self.anomaly_preds = np.zeros(len(self.test_data)) if self.test_data is not None else None\n",
    "        self.anomaly_errors = np.zeros(len(self.test_data)) if self.test_data is not None else None\n",
    "        self.predictions = np.zeros(len(self.test_data)) if self.test_data is not None else None\n",
    "\n",
    "        self.losses = {'train': [], 'valid': []}\n",
    "        self.name = 'LSTMAutoencoder'  # A name attribute for the class\n",
    "\n",
    "        set_seed(seed)\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = tf.keras.Input(shape=(self.timesteps, self.features), name='input_layer')\n",
    "\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True, name='lstm_1')(inputs)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=False, name='latent')(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.timesteps, name='repeat_vector')(x)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=True, name='lstm_3')(x)\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True, name='lstm_4')(x)\n",
    "        outputs = layers.TimeDistributed(layers.Dense(self.features, name='dense_output'))(x)\n",
    "\n",
    "        self.model = models.Model(inputs, outputs, name='model')\n",
    "\n",
    "    def compute_threshold(self):\n",
    "        if self.train_data_window is None:\n",
    "            print(\"No training windows to compute threshold.\")\n",
    "            self.threshold = 9999999\n",
    "            return\n",
    "        rec = self.model.predict(self.train_data_window, verbose=0)\n",
    "        mse = np.mean(np.square(self.train_data_window - rec), axis=(1, 2))\n",
    "        self.threshold = np.mean(mse) + self.threshold_sigma * np.std(mse)\n",
    "\n",
    "    def train(self,\n",
    "              batch_size=32,\n",
    "              epochs=50,\n",
    "              optimizer='adam',\n",
    "              loss='mse',\n",
    "              patience=10,\n",
    "              shuffle: bool = False,\n",
    "              seed: int = 42):\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Custom max-diff loss function\n",
    "        def max_diff_loss(y_true, y_pred):\n",
    "            # return the average of (max(|x - recon|) across timesteps)\n",
    "            # so itâ€™s a single scalar\n",
    "            return tf.reduce_mean(tf.reduce_max(tf.abs(y_true - y_pred), axis=[1, 2]))\n",
    "\n",
    "        # Determine which loss function to use\n",
    "        loss_function = 'mse' if loss == 'mse' else max_diff_loss\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "        if self.train_data_window is None:\n",
    "            print(\"No training windows found. Skipping training.\")\n",
    "            return\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            self.train_data_window, self.train_data_window,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            validation_split=0.1,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        self.losses['train'] = [float(l) for l in history.history['loss']]\n",
    "        self.losses['valid'] = [float(l) for l in history.history['val_loss']]\n",
    "\n",
    "    def evaluate(self, batch_size=32, loss='mse'):\n",
    "        \"\"\"\n",
    "        Evaluate the model on self.test_data_window.\n",
    "        Sets self.anomaly_preds, self.anomaly_errors, self.predictions.\n",
    "        \"\"\"\n",
    "        if self.test_data_window is None or len(self.test_data_window) == 0:\n",
    "            print(\"No test windows available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        length = self.test_data.shape[0]\n",
    "        self.compute_threshold()\n",
    "\n",
    "        # Generate predictions for the test data windows\n",
    "        self.predictions_windows = self.model.predict(self.test_data_window, batch_size=batch_size)\n",
    "\n",
    "        # Compute reconstruction errors\n",
    "        if loss == 'mse':\n",
    "            errors = np.mean(np.square(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "        else:\n",
    "            errors = np.max(np.abs(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "\n",
    "        # Expand window errors to match original time steps\n",
    "        M = errors.shape[0]\n",
    "        timestep_errors = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            timestep_errors[start:end + 1] += errors[i]\n",
    "            counts[start:end + 1] += 1\n",
    "\n",
    "        counts[counts == 0] = 1  # Avoid division by zero\n",
    "        timestep_errors /= counts  # Average overlapping windows\n",
    "\n",
    "        self.anomaly_preds = (timestep_errors > self.threshold).astype(int)\n",
    "        self.anomaly_errors = timestep_errors\n",
    "\n",
    "        # Compute predictions (averaged across windows)\n",
    "        counts = np.zeros(length)\n",
    "        self.predictions = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                timestep_index = i + j\n",
    "                if timestep_index < length:\n",
    "                    self.predictions[timestep_index] += self.predictions_windows[i, j]\n",
    "                    counts[timestep_index] += 1\n",
    "\n",
    "        # Avoid division by zero\n",
    "        for i in range(length):\n",
    "            if counts[i] > 0:\n",
    "                self.predictions[i] /= counts[i]\n",
    "\n",
    "        self.predictions = np.nan_to_num(self.predictions)\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        \"\"\"\n",
    "        Returns latent representation from the encoder part of the model.\n",
    "        \"\"\"\n",
    "        encoder_model = models.Model(inputs=self.model.input,\n",
    "                                     outputs=self.model.get_layer('latent').output)\n",
    "        latent_representations = encoder_model.predict(x)\n",
    "        return latent_representations\n",
    "\n",
    "    def save_model(self, model_path: str = \"model.h5\"):\n",
    "        \"\"\"\n",
    "        Save the Keras model to disk.\n",
    "        \"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            print(\"No model to save.\")\n",
    "\n",
    "    def load_model(self, model_path: str, train_path: str, test_path: str, label_path: str):\n",
    "        \"\"\"\n",
    "        Load the Keras model from the specified file paths and set\n",
    "        self.train_data, self.test_data, and self.labels accordingly.\n",
    "        \"\"\"\n",
    "        self.model = models.load_model(model_path, compile=False)\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "        # Load data\n",
    "        self.train_data = np.load(train_path).astype(np.float32)\n",
    "        self.test_data = np.load(test_path).astype(np.float32)\n",
    "        self.labels = np.load(label_path).astype(int)\n",
    "\n",
    "        # Recreate windows\n",
    "        self.train_data_window = create_windows(self.train_data, self.timesteps, self.step_size)\n",
    "        self.test_data_window = create_windows(self.test_data, self.timesteps, 1)\n",
    "\n",
    "        print(f\"Loaded model from {model_path} and data from {train_path}, {test_path}, {label_path}.\")\n",
    "\n",
    "    def plot_results(self, save_path=None, file_format='html', size=800):\n",
    "        \"\"\"\n",
    "        Plot test data, predictions, anomaly errors, and highlight\n",
    "        labeled anomalies and predicted anomalies.\n",
    "        \"\"\"\n",
    "        if self.test_data is None or self.labels is None:\n",
    "            print(\"No test data or labels to plot.\")\n",
    "            return\n",
    "\n",
    "        # Flatten arrays\n",
    "        test_data = self.test_data.ravel()\n",
    "        anomaly_preds = self.anomaly_preds\n",
    "        anomaly_errors = self.anomaly_errors\n",
    "        predictions = self.predictions\n",
    "        labels = self.labels.ravel()\n",
    "\n",
    "        if not (len(test_data) == len(labels) == len(anomaly_preds) == len(anomaly_errors) == len(predictions)):\n",
    "            raise ValueError(\"All input arrays must have the same length.\")\n",
    "\n",
    "        plot_width = max(size, len(test_data) // 10)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        # Test Data\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        # Predictions\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(predictions))),\n",
    "                                 y=predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        # Anomaly Errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(anomaly_errors))),\n",
    "                                 y=anomaly_errors,\n",
    "                                 mode='lines',\n",
    "                                 name='Anomaly Errors',\n",
    "                                 line=dict(color='red')))\n",
    "\n",
    "        # Labeled anomalies\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels on Test Data',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Predicted anomalies\n",
    "        anomaly_pred_indices = [i for i in range(len(anomaly_preds)) if anomaly_preds[i] == 1]\n",
    "        if anomaly_pred_indices:\n",
    "            fig.add_trace(go.Scatter(x=anomaly_pred_indices,\n",
    "                                     y=[predictions[i] for i in anomaly_pred_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Anomaly Predictions',\n",
    "                                     marker=dict(color='green', size=10)))\n",
    "\n",
    "        fig.update_layout(title='Test Data, Predictions, and Anomalies',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "\n",
    "        # Optionally save the figure\n",
    "        if save_path is not None:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            if file_format.lower() == 'html':\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    def plot_losses(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot training and validation losses.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses['train'], label='Training Loss')\n",
    "        plt.plot(self.losses['valid'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3) MixtureOfExpertsLSTMAutoencoder Class (With Plotting for Expert1, Expert2, and MOE Combined)\n",
    "# ==============================\n",
    "\n",
    "class MixtureOfExpertsLSTMAutoencoder:\n",
    "    \"\"\"\n",
    "    Two LSTMAutoencoder experts (expert1, expert2) with:\n",
    "      - identical architecture\n",
    "      - separate trainable weights and losses\n",
    "      - dynamic gating threshold (for expert1) to decide if a window is passed on to expert2\n",
    "      - separate dynamic threshold for expert2, computed from the windows that actually pass\n",
    "      - dedicated plot functions for:\n",
    "          1) Expert1 alone\n",
    "          2) Expert2 alone\n",
    "          3) Final MoE combination\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 labels,\n",
    "                 timesteps=128,\n",
    "                 features=1,\n",
    "                 latent_dim=32,\n",
    "                 lstm_units=64,\n",
    "                 step_size=1,\n",
    "                 threshold_sigma=2.0,\n",
    "                 seed=0,\n",
    "                 loss='mse'):\n",
    "        self.train_data = train_data.astype(np.float32) if train_data is not None else None\n",
    "        self.test_data = test_data.astype(np.float32) if test_data is not None else None\n",
    "        self.labels = labels.astype(int) if labels is not None else None\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.step_size = step_size\n",
    "        self.threshold_sigma = threshold_sigma\n",
    "        self.seed = seed\n",
    "        self.loss_str = loss  # store 'mse' or 'max_diff_loss'\n",
    "\n",
    "        # Prepare windowed data\n",
    "        self.train_windows = create_windows(self.train_data, timesteps, step_size)\n",
    "        self.test_windows = create_windows(self.test_data, timesteps, 1)\n",
    "\n",
    "        # Build two separate sub-models\n",
    "        self.expert1 = self._build_expert()\n",
    "        self.expert2 = self._build_expert()\n",
    "\n",
    "        # Dynamic thresholds\n",
    "        self.dynamic_threshold_e1 = 0.0\n",
    "        self.dynamic_threshold_e2 = 0.0\n",
    "\n",
    "        # Final time-series outputs after gating\n",
    "        self.final_anomaly_score = None\n",
    "        self.anomaly_preds = None\n",
    "\n",
    "        # If we want to store pure \"expert1 only\" or \"expert2 only\" reconstructions:\n",
    "        self.e1_time_scores = None\n",
    "        self.e1_time_preds = None\n",
    "        self.e2_time_scores = None\n",
    "        self.e2_time_preds = None\n",
    "\n",
    "        set_seed(seed)\n",
    "\n",
    "    def _build_expert(self):\n",
    "        inputs = tf.keras.Input(shape=(self.timesteps, self.features))\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True)(inputs)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=False, name='latent')(x)\n",
    "        x = layers.RepeatVector(self.timesteps)(x)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=True)(x)\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True)(x)\n",
    "        outputs = layers.TimeDistributed(layers.Dense(self.features))(x)\n",
    "        return models.Model(inputs, outputs)\n",
    "\n",
    "    def _get_loss_fn(self):\n",
    "        \"\"\"\n",
    "        Returns MSE or max_diff_loss function for reconstruction.\n",
    "        \"\"\"\n",
    "        if self.loss_str == 'mse':\n",
    "            return tf.keras.losses.MeanSquaredError()\n",
    "        else:\n",
    "            def max_diff_loss(y_true, y_pred):\n",
    "                return tf.reduce_mean(tf.reduce_max(tf.abs(y_true - y_pred), axis=[1, 2]))\n",
    "            return max_diff_loss\n",
    "\n",
    "    def train(self, epochs=50, batch_size=32, patience=10, optimizer='adam'):\n",
    "        \"\"\"\n",
    "        Custom training loop:\n",
    "          - Each epoch:\n",
    "              1) For each batch, get expert1 recon error.\n",
    "              2) If error > dynamic_threshold_e1, pass subset to expert2.\n",
    "              3) Update both experts accordingly.\n",
    "              4) Recompute dynamic_threshold_e1 after each epoch from training set.\n",
    "        \"\"\"\n",
    "        if self.train_windows is None:\n",
    "            print(\"No training windows available. Cannot train.\")\n",
    "            return\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.train_windows).batch(batch_size, drop_remainder=False)\n",
    "\n",
    "        loss_fn = self._get_loss_fn()\n",
    "        opt1 = tf.keras.optimizers.get(optimizer)\n",
    "        opt2 = tf.keras.optimizers.get(optimizer)\n",
    "\n",
    "        best_loss = np.inf\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Initialize threshold so that in epoch 0, we pass everything\n",
    "        self.dynamic_threshold_e1 = -999999\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses_e1 = []\n",
    "            epoch_losses_e2 = []\n",
    "\n",
    "            for x_batch in dataset:\n",
    "                x_batch = tf.cast(x_batch, tf.float32)\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    # Expert1 forward\n",
    "                    recon1 = self.expert1(x_batch, training=True)\n",
    "                    if self.loss_str == 'mse':\n",
    "                        e1_errors = tf.reduce_mean(tf.square(x_batch - recon1), axis=[1,2])\n",
    "                    else:\n",
    "                        e1_errors = tf.reduce_max(tf.abs(x_batch - recon1), axis=[1,2])\n",
    "                    loss_e1 = loss_fn(x_batch, recon1)\n",
    "\n",
    "                    # Gating: pass only windows with e1_errors > threshold\n",
    "                    pass_mask = tf.greater(e1_errors, self.dynamic_threshold_e1)\n",
    "                    x_pass_e2 = tf.boolean_mask(x_batch, pass_mask)\n",
    "                    \n",
    "                    if tf.shape(x_pass_e2)[0] > 0:\n",
    "                        recon2 = self.expert2(x_pass_e2, training=True)\n",
    "                        loss_e2 = loss_fn(x_pass_e2, recon2)\n",
    "                    else:\n",
    "                        loss_e2 = 0.0\n",
    "\n",
    "                # Backprop\n",
    "                grads1 = tape.gradient(loss_e1, self.expert1.trainable_weights)\n",
    "                opt1.apply_gradients(zip(grads1, self.expert1.trainable_weights))\n",
    "\n",
    "                if tf.shape(x_pass_e2)[0] > 0:\n",
    "                    grads2 = tape.gradient(loss_e2, self.expert2.trainable_weights)\n",
    "                    opt2.apply_gradients(zip(grads2, self.expert2.trainable_weights))\n",
    "\n",
    "                del tape\n",
    "\n",
    "                epoch_losses_e1.append(loss_e1.numpy())\n",
    "                if tf.shape(x_pass_e2)[0] > 0:\n",
    "                    epoch_losses_e2.append(loss_e2.numpy())\n",
    "                else:\n",
    "                    epoch_losses_e2.append(0.0)\n",
    "\n",
    "            # Update dynamic_threshold_e1 after each epoch\n",
    "            # Forward pass entire train set with expert1\n",
    "            e1_all = []\n",
    "            for x_batch_all in dataset:\n",
    "                recon_all = self.expert1(x_batch_all, training=False)\n",
    "                if self.loss_str == 'mse':\n",
    "                    batch_e1 = tf.reduce_mean(tf.square(x_batch_all - recon_all), axis=[1,2])\n",
    "                else:\n",
    "                    batch_e1 = tf.reduce_max(tf.abs(x_batch_all - recon_all), axis=[1,2])\n",
    "                e1_all.extend(batch_e1.numpy())\n",
    "            e1_all = np.array(e1_all)\n",
    "            mean_e1 = np.mean(e1_all)\n",
    "            std_e1 = np.std(e1_all)\n",
    "            self.dynamic_threshold_e1 = mean_e1 + self.threshold_sigma * std_e1\n",
    "\n",
    "            avg_e1 = np.mean(epoch_losses_e1)\n",
    "            avg_e2 = np.mean(epoch_losses_e2)\n",
    "            combined_loss = avg_e1 + avg_e2\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} \"\n",
    "                  f\"- E1 Loss: {avg_e1:.4f} \"\n",
    "                  f\"- E2 Loss: {avg_e2:.4f} \"\n",
    "                  f\"- dynamic_threshold_e1: {self.dynamic_threshold_e1:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if combined_loss < best_loss:\n",
    "                best_loss = combined_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        # Compute dynamic threshold for expert2\n",
    "        self._compute_expert2_threshold(dataset)\n",
    "\n",
    "    def _compute_expert2_threshold(self, dataset):\n",
    "        \"\"\"\n",
    "        Gathers all training windows that pass e1 threshold, then gets e2 errors,\n",
    "        sets dynamic_threshold_e2 = mean + sigma * std\n",
    "        \"\"\"\n",
    "        e2_list = []\n",
    "        for x_batch in dataset:\n",
    "            recon1 = self.expert1(x_batch, training=False)\n",
    "            if self.loss_str == 'mse':\n",
    "                e1_errors = tf.reduce_mean(tf.square(x_batch - recon1), axis=[1,2])\n",
    "            else:\n",
    "                e1_errors = tf.reduce_max(tf.abs(x_batch - recon1), axis=[1,2])\n",
    "            pass_mask = tf.greater(e1_errors, self.dynamic_threshold_e1)\n",
    "            x_pass_e2 = tf.boolean_mask(x_batch, pass_mask)\n",
    "\n",
    "            if tf.shape(x_pass_e2)[0] > 0:\n",
    "                recon2 = self.expert2(x_pass_e2, training=False)\n",
    "                if self.loss_str == 'mse':\n",
    "                    e2_err = tf.reduce_mean(tf.square(x_pass_e2 - recon2), axis=[1,2])\n",
    "                else:\n",
    "                    e2_err = tf.reduce_max(tf.abs(x_pass_e2 - recon2), axis=[1,2])\n",
    "                e2_list.extend(e2_err.numpy())\n",
    "\n",
    "        if len(e2_list) == 0:\n",
    "            self.dynamic_threshold_e2 = 999999.0\n",
    "        else:\n",
    "            mean_e2 = np.mean(e2_list)\n",
    "            std_e2 = np.std(e2_list)\n",
    "            self.dynamic_threshold_e2 = mean_e2 + self.threshold_sigma * std_e2\n",
    "\n",
    "    def evaluate(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Final gating-based evaluation:\n",
    "          1) If e1_error <= threshold => normal (use e1)\n",
    "             else => pass to e2 => if e2_error > e2_threshold => anomaly, else normal.\n",
    "          2) Expand window-level decisions to time steps -> self.final_anomaly_score, self.anomaly_preds\n",
    "        \"\"\"\n",
    "        if self.test_windows is None:\n",
    "            print(\"No test windows for evaluation.\")\n",
    "            return\n",
    "\n",
    "        # We store final error for each window, final binary label for each window\n",
    "        window_scores = []\n",
    "        window_labels = []\n",
    "\n",
    "        ds_test = tf.data.Dataset.from_tensor_slices(self.test_windows).batch(batch_size)\n",
    "        for x_batch in ds_test:\n",
    "            recon1 = self.expert1(x_batch, training=False)\n",
    "            if self.loss_str == 'mse':\n",
    "                e1_err = tf.reduce_mean(tf.square(x_batch - recon1), axis=[1,2])\n",
    "            else:\n",
    "                e1_err = tf.reduce_max(tf.abs(x_batch - recon1), axis=[1,2])\n",
    "\n",
    "            pass_mask = tf.greater(e1_err, self.dynamic_threshold_e1)\n",
    "            x_e2 = tf.boolean_mask(x_batch, pass_mask)\n",
    "\n",
    "            if tf.shape(x_e2)[0] > 0:\n",
    "                recon2 = self.expert2(x_e2, training=False)\n",
    "                if self.loss_str == 'mse':\n",
    "                    e2_err = tf.reduce_mean(tf.square(x_e2 - recon2), axis=[1,2])\n",
    "                else:\n",
    "                    e2_err = tf.reduce_max(tf.abs(x_e2 - recon2), axis=[1,2])\n",
    "\n",
    "                # reinsert into correct positions\n",
    "                idx_e2 = 0\n",
    "                e2_full = []\n",
    "                pass_mask_np = pass_mask.numpy()\n",
    "                for pm in pass_mask_np:\n",
    "                    if pm:\n",
    "                        e2_full.append(e2_err[idx_e2].numpy())\n",
    "                        idx_e2 += 1\n",
    "                    else:\n",
    "                        e2_full.append(0.0)\n",
    "            else:\n",
    "                e2_full = [0.0]*len(pass_mask)\n",
    "\n",
    "            # Build final error + label\n",
    "            for i in range(len(e1_err)):\n",
    "                if not pass_mask[i]:\n",
    "                    # e1 says normal => final error = e1_err\n",
    "                    window_scores.append(e1_err[i].numpy())\n",
    "                    window_labels.append(0)\n",
    "                else:\n",
    "                    # pass to e2\n",
    "                    # if e2_full[i] > dynamic_threshold_e2 => anomaly\n",
    "                    if e2_full[i] > self.dynamic_threshold_e2:\n",
    "                        window_labels.append(1)\n",
    "                    else:\n",
    "                        window_labels.append(0)\n",
    "                    window_scores.append(e2_full[i])\n",
    "\n",
    "        # Expand window_scores, window_labels to time steps\n",
    "        length = self.test_data.shape[0]\n",
    "        M = len(self.test_windows)\n",
    "        time_scores = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            time_scores[start:end+1] += window_scores[i]\n",
    "            counts[start:end+1] += 1\n",
    "\n",
    "        counts[counts == 0] = 1\n",
    "        time_scores /= counts\n",
    "\n",
    "        time_preds = np.zeros(length, dtype=int)\n",
    "        for i in range(M):\n",
    "            if window_labels[i] == 1:\n",
    "                start = i\n",
    "                end = i + self.timesteps - 1\n",
    "                time_preds[start:end+1] = 1\n",
    "\n",
    "        self.final_anomaly_score = time_scores\n",
    "        self.anomaly_preds = time_preds\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Plot Expert1 alone (all windows -> expert1) ignoring gating\n",
    "    # -------------------------------------------------------------------------\n",
    "    def plot_expert1_results(self, save_path=None, file_format='html', size=800):\n",
    "        \"\"\"\n",
    "        Evaluate expert1 alone on all test windows (ignoring gating) using\n",
    "        the final threshold_e1. Plots the resulting reconstructions, errors, anomalies.\n",
    "        \"\"\"\n",
    "        if self.test_windows is None or self.labels is None:\n",
    "            print(\"No test windows or labels to plot for expert1.\")\n",
    "            return\n",
    "\n",
    "        # Recompute threshold_e1 if not computed\n",
    "        if self.dynamic_threshold_e1 == 0.0:\n",
    "            print(\"Warning: dynamic_threshold_e1 not set. Expert1 may not be trained or threshold not updated.\")\n",
    "            self.dynamic_threshold_e1 = 999999.0\n",
    "\n",
    "        # 1) reconstruct all test windows with expert1\n",
    "        e1_recon = self.expert1.predict(self.test_windows)\n",
    "        # 2) compute errors\n",
    "        if self.loss_str == 'mse':\n",
    "            window_errors = np.mean(np.square(self.test_windows - e1_recon), axis=(1,2))\n",
    "        else:\n",
    "            window_errors = np.max(np.abs(self.test_windows - e1_recon), axis=(1,2))\n",
    "\n",
    "        # 3) expand to time steps\n",
    "        length = self.test_data.shape[0]\n",
    "        M = len(window_errors)\n",
    "        time_scores = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            time_scores[start:end+1] += window_errors[i]\n",
    "            counts[start:end+1] += 1\n",
    "        counts[counts == 0] = 1\n",
    "        time_scores /= counts\n",
    "\n",
    "        # 4) get anomaly preds\n",
    "        time_preds = (time_scores > self.dynamic_threshold_e1).astype(int)\n",
    "\n",
    "        # 5) reconstruct predictions (averaging across windows) just like LSTMAE\n",
    "        time_predictions = np.zeros(length)\n",
    "        counts_pred = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                idx = i + j\n",
    "                if idx < length:\n",
    "                    time_predictions[idx] += e1_recon[i, j]\n",
    "                    counts_pred[idx] += 1\n",
    "        for i in range(length):\n",
    "            if counts_pred[i] > 0:\n",
    "                time_predictions[i] /= counts_pred[i]\n",
    "\n",
    "        # 6) plot\n",
    "        test_data = self.test_data.ravel()\n",
    "        labels = self.labels.ravel()\n",
    "\n",
    "        plot_width = max(size, len(test_data)//10)\n",
    "        fig = go.Figure()\n",
    "        # Test Data\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        # Expert1 Predictions\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(time_predictions))),\n",
    "                                 y=time_predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Expert1 Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        # E1 Errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(time_scores))),\n",
    "                                 y=time_scores,\n",
    "                                 mode='lines',\n",
    "                                 name='Expert1 Errors',\n",
    "                                 line=dict(color='red')))\n",
    "\n",
    "        # Labeled anomalies\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Pred anomalies\n",
    "        anomaly_indices = [i for i in range(len(time_preds)) if time_preds[i] == 1]\n",
    "        if anomaly_indices:\n",
    "            fig.add_trace(go.Scatter(x=anomaly_indices,\n",
    "                                     y=[time_predictions[i] for i in anomaly_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Anomaly Preds (E1)',\n",
    "                                     marker=dict(color='green', size=10)))\n",
    "\n",
    "        fig.update_layout(title='Expert1 Alone Results',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "\n",
    "        if save_path is not None:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            if file_format.lower() == 'html':\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "            print(f\"[Expert1] Plot saved to: {save_path}\")\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Plot Expert2 alone (all windows -> expert2), ignoring gating\n",
    "    # -------------------------------------------------------------------------\n",
    "    def plot_expert2_results(self, save_path=None, file_format='html', size=800):\n",
    "        \"\"\"\n",
    "        Evaluate expert2 alone on all test windows (ignoring gating) using\n",
    "        the final threshold_e2. Plots the resulting reconstructions, errors, anomalies.\n",
    "        \"\"\"\n",
    "        if self.test_windows is None or self.labels is None:\n",
    "            print(\"No test windows or labels to plot for expert2.\")\n",
    "            return\n",
    "\n",
    "        if self.dynamic_threshold_e2 == 0.0:\n",
    "            print(\"Warning: dynamic_threshold_e2 not set. Expert2 may not be trained or threshold not updated.\")\n",
    "            self.dynamic_threshold_e2 = 999999.0\n",
    "\n",
    "        # 1) reconstruct all test windows with expert2\n",
    "        e2_recon = self.expert2.predict(self.test_windows)\n",
    "        # 2) compute errors\n",
    "        if self.loss_str == 'mse':\n",
    "            window_errors = np.mean(np.square(self.test_windows - e2_recon), axis=(1,2))\n",
    "        else:\n",
    "            window_errors = np.max(np.abs(self.test_windows - e2_recon), axis=(1,2))\n",
    "\n",
    "        # 3) expand to time steps\n",
    "        length = self.test_data.shape[0]\n",
    "        M = len(window_errors)\n",
    "        time_scores = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            time_scores[start:end+1] += window_errors[i]\n",
    "            counts[start:end+1] += 1\n",
    "        counts[counts == 0] = 1\n",
    "        time_scores /= counts\n",
    "\n",
    "        # 4) get anomaly preds\n",
    "        time_preds = (time_scores > self.dynamic_threshold_e2).astype(int)\n",
    "\n",
    "        # 5) reconstruct predictions\n",
    "        time_predictions = np.zeros(length)\n",
    "        counts_pred = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                idx = i + j\n",
    "                if idx < length:\n",
    "                    time_predictions[idx] += e2_recon[i, j]\n",
    "                    counts_pred[idx] += 1\n",
    "        for i in range(length):\n",
    "            if counts_pred[i] > 0:\n",
    "                time_predictions[i] /= counts_pred[i]\n",
    "\n",
    "        # 6) plot\n",
    "        test_data = self.test_data.ravel()\n",
    "        labels = self.labels.ravel()\n",
    "\n",
    "        plot_width = max(size, len(test_data)//10)\n",
    "        fig = go.Figure()\n",
    "        # Test Data\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        # Expert2 Predictions\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(time_predictions))),\n",
    "                                 y=time_predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Expert2 Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        # E2 Errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(time_scores))),\n",
    "                                 y=time_scores,\n",
    "                                 mode='lines',\n",
    "                                 name='Expert2 Errors',\n",
    "                                 line=dict(color='red')))\n",
    "\n",
    "        # Labeled anomalies\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Pred anomalies\n",
    "        anomaly_indices = [i for i in range(len(time_preds)) if time_preds[i] == 1]\n",
    "        if anomaly_indices:\n",
    "            fig.add_trace(go.Scatter(x=anomaly_indices,\n",
    "                                     y=[time_predictions[i] for i in anomaly_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Anomaly Preds (E2)',\n",
    "                                     marker=dict(color='green', size=10)))\n",
    "\n",
    "        fig.update_layout(title='Expert2 Alone Results',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "\n",
    "        if save_path is not None:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            if file_format.lower() == 'html':\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "            print(f\"[Expert2] Plot saved to: {save_path}\")\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Plot final MoE gating results (the combined approach from self.evaluate())\n",
    "    # -------------------------------------------------------------------------\n",
    "    def plot_moe_final_results(self, save_path=None, file_format='html', size=800):\n",
    "        \"\"\"\n",
    "        Plots the final gating-based reconstruction and anomalies after calling self.evaluate().\n",
    "        This uses self.final_anomaly_score and self.anomaly_preds.\n",
    "        For each window:\n",
    "            if e1_err <= threshold => use e1 recon\n",
    "            else => use e2 recon\n",
    "        Expanded to time steps, we show final anomalies and reconstruction.\n",
    "        \"\"\"\n",
    "        if self.final_anomaly_score is None or self.anomaly_preds is None:\n",
    "            print(\"Please run .evaluate() first to get final anomaly scores & preds.\")\n",
    "            return\n",
    "\n",
    "        if self.test_windows is None or self.labels is None:\n",
    "            print(\"No test windows or labels to plot for MOE final.\")\n",
    "            return\n",
    "\n",
    "        # Let's reconstruct the final \"MoE reconstruction\" at the window level\n",
    "        # For each window i, if e1_err <= e1 threshold => e1 recon\n",
    "        # else => e2 recon. Then expand to time steps.\n",
    "\n",
    "        # We'll replicate the gating logic from evaluate, storing the actual reconstruction.\n",
    "        ds_test = tf.data.Dataset.from_tensor_slices(self.test_windows).batch(32)\n",
    "        window_recons = []\n",
    "        for x_batch in ds_test:\n",
    "            recon1 = self.expert1(x_batch, training=False)\n",
    "            if self.loss_str == 'mse':\n",
    "                e1_err = tf.reduce_mean(tf.square(x_batch - recon1), axis=[1,2])\n",
    "            else:\n",
    "                e1_err = tf.reduce_max(tf.abs(x_batch - recon1), axis=[1,2])\n",
    "\n",
    "            pass_mask = tf.greater(e1_err, self.dynamic_threshold_e1)\n",
    "            x_e2 = tf.boolean_mask(x_batch, pass_mask)\n",
    "            recon2 = None\n",
    "            if tf.shape(x_e2)[0] > 0:\n",
    "                recon2 = self.expert2(x_e2, training=False)\n",
    "\n",
    "            # we must re-insert recon2 into the correct windows\n",
    "            idx_e2 = 0\n",
    "            combined_batch_recons = []\n",
    "            for i in range(len(x_batch)):\n",
    "                if not pass_mask[i]:\n",
    "                    # Use recon1\n",
    "                    combined_batch_recons.append(recon1[i].numpy())\n",
    "                else:\n",
    "                    # Use recon2\n",
    "                    if recon2 is not None:\n",
    "                        combined_batch_recons.append(recon2[idx_e2].numpy())\n",
    "                        idx_e2 += 1\n",
    "                    else:\n",
    "                        # fallback\n",
    "                        combined_batch_recons.append(recon1[i].numpy())\n",
    "            window_recons.extend(combined_batch_recons)\n",
    "\n",
    "        # Now expand to time steps\n",
    "        length = self.test_data.shape[0]\n",
    "        M = len(self.test_windows)\n",
    "        final_reconstruction_ts = np.zeros((length, self.features))\n",
    "        counts = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            recon_window = window_recons[i]  # shape (timesteps, features)\n",
    "            for j in range(self.timesteps):\n",
    "                idx = start + j\n",
    "                if idx < length:\n",
    "                    final_reconstruction_ts[idx] += recon_window[j]\n",
    "                    counts[idx] += 1\n",
    "        for i in range(length):\n",
    "            if counts[i] > 0:\n",
    "                final_reconstruction_ts[i] /= counts[i]\n",
    "\n",
    "        # flatten for plotting if univariate\n",
    "        if self.features == 1:\n",
    "            final_reconstruction_ts = final_reconstruction_ts.ravel()\n",
    "\n",
    "        time_scores = self.final_anomaly_score\n",
    "        time_preds = self.anomaly_preds\n",
    "        test_data = self.test_data.ravel()\n",
    "        labels = self.labels.ravel()\n",
    "\n",
    "        plot_width = max(size, len(test_data)//10)\n",
    "        fig = go.Figure()\n",
    "        # Test Data\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        # MOE Combined Predictions\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(final_reconstruction_ts))),\n",
    "                                 y=final_reconstruction_ts,\n",
    "                                 mode='lines',\n",
    "                                 name='MoE Combined Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        # MoE final anomaly errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(time_scores))),\n",
    "                                 y=time_scores,\n",
    "                                 mode='lines',\n",
    "                                 name='MoE Anomaly Errors',\n",
    "                                 line=dict(color='red')))\n",
    "\n",
    "        # Labeled anomalies\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Predicted anomalies\n",
    "        anomaly_indices = [i for i in range(len(time_preds)) if time_preds[i] == 1]\n",
    "        if anomaly_indices:\n",
    "            fig.add_trace(go.Scatter(x=anomaly_indices,\n",
    "                                     y=[final_reconstruction_ts[i] for i in anomaly_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Anomaly Preds (MoE)',\n",
    "                                     marker=dict(color='green', size=10)))\n",
    "\n",
    "        fig.update_layout(title='MoE Final Results (Gating)',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "\n",
    "        if save_path is not None:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            if file_format.lower() == 'html':\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "            print(f\"[MoE Final] Plot saved to: {save_path}\")\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Save/Load sub-models\n",
    "    # -------------------------------------------------------------------------\n",
    "    def save_models(self, dir_path=\"moe_models\"):\n",
    "        \"\"\"\n",
    "        Saves the expert1 and expert2 models in H5 format.\n",
    "        \"\"\"\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        self.expert1.save(os.path.join(dir_path, \"expert1.h5\"))\n",
    "        self.expert2.save(os.path.join(dir_path, \"expert2.h5\"))\n",
    "        print(f\"Saved MoE experts to {dir_path}/expert1.h5 and {dir_path}/expert2.h5\")\n",
    "\n",
    "    def load_models(self, dir_path, train_path=None, test_path=None, label_path=None):\n",
    "        \"\"\"\n",
    "        Loads expert1 and expert2 from H5 files. Also re-loads data if paths given.\n",
    "        \"\"\"\n",
    "        expert1_path = os.path.join(dir_path, \"expert1.h5\")\n",
    "        expert2_path = os.path.join(dir_path, \"expert2.h5\")\n",
    "        self.expert1 = models.load_model(expert1_path, compile=False)\n",
    "        self.expert2 = models.load_model(expert2_path, compile=False)\n",
    "        self.expert1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        self.expert2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        if train_path and test_path and label_path:\n",
    "            self.train_data = np.load(train_path).astype(np.float32)\n",
    "            self.test_data = np.load(test_path).astype(np.float32)\n",
    "            self.labels = np.load(label_path).astype(int)\n",
    "            self.train_windows = create_windows(self.train_data, self.timesteps, self.step_size)\n",
    "            self.test_windows = create_windows(self.test_data, self.timesteps, 1)\n",
    "        print(f\"Loaded MoE experts from {expert1_path} and {expert2_path}.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4) Universal Evaluation Function (extended for MoE)\n",
    "# ==============================\n",
    "\n",
    "def evaluate_model_and_save_results(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    results_csv_path,\n",
    "    # For LSTMAutoencoder or StationaryLSTMAutoencoder:\n",
    "    train_path=None,\n",
    "    test_path=None,\n",
    "    label_path=None,\n",
    "    loss_type='mse',\n",
    "    # For TransformerAE (PyTorch):\n",
    "    test_loader=None,\n",
    "    train_data=None,\n",
    "    test_data=None,\n",
    "    labels=None,\n",
    "    # For MixtureOfExpertsLSTMAutoencoder:\n",
    "    moe_dir_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates one of these classes:\n",
    "      - LSTMAutoencoder (Keras)\n",
    "      - StationaryLSTMAutoencoder (Keras)\n",
    "      - TransformerAE (PyTorch)\n",
    "      - MixtureOfExpertsLSTMAutoencoder\n",
    "    and saves results (AUC metrics, F1, etc.) to CSV.\n",
    "    \"\"\"\n",
    "    # CASE 1: LSTMAutoencoder\n",
    "    if model_class.__name__ == \"LSTMAutoencoder\":\n",
    "        if not (train_path and test_path and label_path):\n",
    "            raise ValueError(\"For LSTMAutoencoder, provide train_path, test_path, label_path.\")\n",
    "\n",
    "        dummy_model = model_class(\n",
    "            train_data=np.array([]),\n",
    "            test_data=np.array([]),\n",
    "            labels=np.array([])\n",
    "        )\n",
    "        dummy_model.load_model(model_path, train_path, test_path, label_path)\n",
    "\n",
    "        # Evaluate using the specified loss_type\n",
    "        dummy_model.evaluate(loss=loss_type)\n",
    "\n",
    "        y_true = dummy_model.labels.flatten()\n",
    "        anomaly_scores = dummy_model.anomaly_errors\n",
    "        y_pred = dummy_model.anomaly_preds\n",
    "\n",
    "    # CASE 2: StationaryLSTMAutoencoder (if used)\n",
    "    elif model_class.__name__ == \"StationaryLSTMAutoencoder\":\n",
    "        raise NotImplementedError(\"Extend for StationaryLSTMAutoencoder if needed.\")\n",
    "\n",
    "    # CASE 3: TransformerAE (PyTorch-based)\n",
    "    elif model_class.__name__ == \"TransformerAE\":\n",
    "        if test_loader is None:\n",
    "            raise ValueError(\"For TransformerAE, please provide test_loader for evaluation.\")\n",
    "        model = model_class.load(model_path)\n",
    "        results = model.predict(test_loader, train=False)\n",
    "        y_true = results['anomalies']\n",
    "        anomaly_scores = results['errors']\n",
    "        if 'predictions' in results:\n",
    "            y_pred = np.array(results['predictions'])\n",
    "        else:\n",
    "            threshold = np.mean(anomaly_scores) + 3.0 * np.std(anomaly_scores)\n",
    "            y_pred = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "    # CASE 4: MixtureOfExpertsLSTMAutoencoder\n",
    "    elif model_class.__name__ == \"MixtureOfExpertsLSTMAutoencoder\":\n",
    "        if not moe_dir_path or not (train_path and test_path and label_path):\n",
    "            raise ValueError(\"For MixtureOfExpertsLSTMAutoencoder, provide moe_dir_path and data paths.\")\n",
    "\n",
    "        dummy_moe = model_class(\n",
    "            train_data=np.array([]),\n",
    "            test_data=np.array([]),\n",
    "            labels=np.array([]),\n",
    "        )\n",
    "        dummy_moe.load_models(moe_dir_path, train_path, test_path, label_path)\n",
    "        dummy_moe.evaluate(batch_size=32)\n",
    "\n",
    "        y_true = dummy_moe.labels.flatten()\n",
    "        anomaly_scores = dummy_moe.final_anomaly_score\n",
    "        y_pred = dummy_moe.anomaly_preds\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model class. \"\n",
    "                         \"Must be LSTMAutoencoder, StationaryLSTMAutoencoder, \"\n",
    "                         \"TransformerAE, or MixtureOfExpertsLSTMAutoencoder.\")\n",
    "\n",
    "    # Compute metrics\n",
    "    custom_auc_val, perfect_point_found = custom_auc_with_perfect_point(y_true, anomaly_scores)\n",
    "    auc_pr_val = compute_auc_pr(y_true, anomaly_scores)\n",
    "    auc_roc_val = compute_auc_roc(y_true, anomaly_scores)\n",
    "    composite_f1_val = composite_f_score(y_true, y_pred)\n",
    "\n",
    "    results_dict = {\n",
    "        \"ModelPath\": model_path,\n",
    "        \"AUC_Custom\": custom_auc_val,\n",
    "        \"PerfectPointFound\": perfect_point_found,\n",
    "        \"AUC_PR\": auc_pr_val,\n",
    "        \"AUC_ROC\": auc_roc_val,\n",
    "        \"CompositeF1\": composite_f1_val\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([results_dict])\n",
    "    df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"Results saved to {results_csv_path}\")\n",
    "    print(\"RESULTS:\")\n",
    "    print(df)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5) Example Usage with MixtureOfExpertsLSTMAutoencoder\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    # run_list = [\"2\", \"37\", \"45\", \"23\", \"43\", \"21\", \"4\", \"8\", \"11\", \"20\", \"50\", \"51\", \"69\", \"31\", \"6\"]\n",
    "    run_list = [\"4\"]\n",
    "    # Adjust accordingly\n",
    "    directory = \"/kaggle/working/anomaly_detection/UCR/UCR2_preprocessed/\"\n",
    "\n",
    "    HYPER_PARAMS = {\n",
    "        \"TIMESTEPS\": 128,\n",
    "        \"LATENT_DIM\": 32,\n",
    "        \"STEP_SIZE\": 10,\n",
    "        \"EPOCHS\": 200,\n",
    "        \"LOSS\": \"mse\"\n",
    "    }\n",
    "\n",
    "    # Prepare results directory\n",
    "    if os.path.exists(\"results\"):\n",
    "        shutil.rmtree(\"results\")\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "    train_files = sorted(glob.glob(os.path.join(directory, \"*_train.npy\")))\n",
    "\n",
    "    for ts in train_files:\n",
    "        ts_id = ts.split(\"/\")[-1].split(\"_\")[0]\n",
    "        if ts_id not in run_list:\n",
    "            continue\n",
    "        print(f\"\\n===========================\\nStarting w/ ts_id = {ts_id}\\n===========================\")\n",
    "\n",
    "        # Identify test/label files\n",
    "        train_file = ts\n",
    "        test_file = ts.replace(\"_train.npy\", \"_test.npy\")\n",
    "        label_file = ts.replace(\"_train.npy\", \"_labels.npy\")\n",
    "\n",
    "        if (not os.path.exists(test_file)) or (not os.path.exists(label_file)):\n",
    "            print(f\"Skipping {ts_id}, missing test/label files.\")\n",
    "            continue\n",
    "\n",
    "        # Load data\n",
    "        X_train = np.load(train_file)\n",
    "        X_test = np.load(test_file)\n",
    "        Y_test = np.load(label_file)\n",
    "\n",
    "        # Instantiate MixtureOfExperts\n",
    "        mixture_model = MixtureOfExpertsLSTMAutoencoder(\n",
    "            train_data=X_train,\n",
    "            test_data=X_test,\n",
    "            labels=Y_test,\n",
    "            timesteps=HYPER_PARAMS[\"TIMESTEPS\"],\n",
    "            features=1,  # assuming univariate\n",
    "            latent_dim=HYPER_PARAMS[\"LATENT_DIM\"],\n",
    "            lstm_units=64,\n",
    "            step_size=HYPER_PARAMS[\"STEP_SIZE\"],\n",
    "            threshold_sigma=2.0,\n",
    "            seed=0,\n",
    "            loss='max_diff_loss' if HYPER_PARAMS[\"LOSS\"] == \"max_diff_loss\" else 'mse'\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        mixture_model.train(\n",
    "            epochs=HYPER_PARAMS[\"EPOCHS\"],\n",
    "            batch_size=32,\n",
    "            patience=10,\n",
    "            optimizer='adam'\n",
    "        )\n",
    "\n",
    "        # Evaluate (final gating approach)\n",
    "        mixture_model.evaluate(batch_size=32)\n",
    "\n",
    "        # Plot Expert1 alone\n",
    "        plot_e1_path = f\"results/moe_{ts_id}_expert1.html\"\n",
    "        mixture_model.plot_expert1_results(save_path=plot_e1_path)\n",
    "\n",
    "        # Plot Expert2 alone\n",
    "        plot_e2_path = f\"results/moe_{ts_id}_expert2.html\"\n",
    "        mixture_model.plot_expert2_results(save_path=plot_e2_path)\n",
    "\n",
    "        # Plot Final gating-based reconstruction\n",
    "        plot_final_path = f\"results/moe_{ts_id}_final.html\"\n",
    "        mixture_model.plot_moe_final_results(save_path=plot_final_path)\n",
    "\n",
    "        # Save the 2 sub-models\n",
    "        moe_dir = f\"results/moe_{ts_id}\"\n",
    "        mixture_model.save_models(moe_dir)\n",
    "\n",
    "        # Evaluate and Save Results to CSV\n",
    "        results_csv = f\"results/moe_{ts_id}_results.csv\"\n",
    "        evaluate_model_and_save_results(\n",
    "            model_class=MixtureOfExpertsLSTMAutoencoder,\n",
    "            model_path=\"(no single path, using moe_dir)\",  # not used for MoE\n",
    "            results_csv_path=results_csv,\n",
    "            train_path=train_file,\n",
    "            test_path=test_file,\n",
    "            label_path=label_file,\n",
    "            loss_type='mse',\n",
    "            moe_dir_path=moe_dir\n",
    "        )\n",
    "\n",
    "    print(\"\\nAll done!\")\n"
   ],
   "id": "bc44e9f4e05807f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
