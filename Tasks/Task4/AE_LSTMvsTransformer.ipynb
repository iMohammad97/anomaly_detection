{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyNvxU3vf4X/mw0SuqoiSqjy"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Packages",
   "metadata": {
    "id": "fB3U8aNVoWTs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install numpy tensorflow scikit-learn plotly pandas\n",
    "\n",
    "# For the UCR dataset we clone the git repo (if in Colab/Kaggle env)\n",
    "!git clone https://github.com/iMohammad97/anomaly_detection\n",
    "\n",
    "!pip install kaleido"
   ],
   "metadata": {
    "id": "jgIUvRdGoWI2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-12T13:23:15.830982Z",
     "iopub.execute_input": "2025-02-12T13:23:15.831286Z",
     "iopub.status.idle": "2025-02-12T13:23:34.646581Z",
     "shell.execute_reply.started": "2025-02-12T13:23:15.831264Z",
     "shell.execute_reply": "2025-02-12T13:23:34.645691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'anomaly_detection'...\nremote: Enumerating objects: 3828, done.\u001B[K\nremote: Counting objects: 100% (30/30), done.\u001B[K\nremote: Compressing objects: 100% (26/26), done.\u001B[K\nremote: Total 3828 (delta 16), reused 8 (delta 4), pack-reused 3798 (from 1)\u001B[K\nReceiving objects: 100% (3828/3828), 202.52 MiB | 33.48 MiB/s, done.\nResolving deltas: 100% (1623/1623), done.\nUpdating files: 100% (2721/2721), done.\nCollecting kaleido\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\nDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.9/79.9 MB\u001B[0m \u001B[31m21.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: kaleido\nSuccessfully installed kaleido-0.2.1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score, precision_score\n",
    "import glob, os, sys\n",
    "import kaleido\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import shutil\n",
    "import math"
   ],
   "metadata": {
    "id": "QlyJs93voTeo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739036175812,
     "user_tz": -210,
     "elapsed": 10718,
     "user": {
      "displayName": "Mohammad Mohammadi",
      "userId": "06409477078097051085"
     }
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-12T13:23:34.647871Z",
     "iopub.execute_input": "2025-02-12T13:23:34.648202Z",
     "iopub.status.idle": "2025-02-12T13:23:53.607582Z",
     "shell.execute_reply.started": "2025-02-12T13:23:34.648154Z",
     "shell.execute_reply": "2025-02-12T13:23:53.606669Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metrics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def pointwise_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Timepoint-wise precision: fraction of detected anomalies that are correct.\n",
    "    \"\"\"\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "def make_event(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Converts binary sequences (y_true and y_pred) into a list of (start, end) event tuples.\n",
    "    \"\"\"\n",
    "    y_true_starts = np.argwhere(np.diff(y_true.flatten(), prepend=0) == 1).flatten()\n",
    "    y_true_ends = np.argwhere(np.diff(y_true.flatten(), append=0) == -1).flatten()\n",
    "    y_true_events = list(zip(y_true_starts, y_true_ends))\n",
    "\n",
    "    y_pred_starts = np.argwhere(np.diff(y_pred, prepend=0) == 1).flatten()\n",
    "    y_pred_ends = np.argwhere(np.diff(y_pred, append=0) == -1).flatten()\n",
    "    y_pred_events = list(zip(y_pred_starts, y_pred_ends))\n",
    "\n",
    "    return y_true_events, y_pred_events\n",
    "\n",
    "\n",
    "def event_wise_recall(y_true_events, y_pred_events):\n",
    "    \"\"\"\n",
    "    Event-based recall. We consider an event 'detected' if the predicted event\n",
    "    overlaps with the true event in any way.\n",
    "    \"\"\"\n",
    "    detected_events = 0\n",
    "    for true_event in y_true_events:\n",
    "        true_start, true_end = true_event\n",
    "        for pred_event in y_pred_events:\n",
    "            pred_start, pred_end = pred_event\n",
    "            if pred_end >= true_start and pred_start <= true_end:\n",
    "                detected_events += 1\n",
    "                break\n",
    "    return detected_events / len(y_true_events) if y_true_events else 0\n",
    "\n",
    "\n",
    "def composite_f_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Combines timepoint precision and event-wise recall into a single F-score.\n",
    "    \"\"\"\n",
    "    prt = pointwise_precision(y_true, y_pred)\n",
    "    y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "    rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "    if prt + rece == 0:\n",
    "        return 0\n",
    "    return 2 * (prt * rece) / (prt + rece)\n",
    "\n",
    "\n",
    "def custom_auc_with_perfect_point(y_true, anomaly_scores, threshold_steps=100, plot=False):\n",
    "    \"\"\"\n",
    "    Generates thresholds, computes precision (timepoint) and recall (event-wise)\n",
    "    pairs, checks for a perfect point, and computes the AUC (area under the curve)\n",
    "    on the PR plane.\n",
    "    \"\"\"\n",
    "    percentiles = np.linspace(np.min(anomaly_scores), np.max(anomaly_scores) + 1e-7, threshold_steps)\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    perfect_point_found = False\n",
    "\n",
    "    for threshold in percentiles:\n",
    "        y_pred = (anomaly_scores >= threshold).astype(int)\n",
    "        prt = pointwise_precision(y_true, y_pred)\n",
    "\n",
    "        y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "        rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "\n",
    "        precision_list.append(prt)\n",
    "        recall_list.append(rece)\n",
    "\n",
    "        if prt == 1 and rece == 1:\n",
    "            perfect_point_found = True\n",
    "            break\n",
    "\n",
    "    # Compute AUC (precision vs recall)\n",
    "    custom_area = auc(recall_list, precision_list)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_list, precision_list, marker='o', label=f\"AUC = {custom_area:.4f}\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    return custom_area, perfect_point_found\n",
    "\n",
    "\n",
    "def compute_auc_pr(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-PR for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, anomaly_scores)\n",
    "        auc_pr = auc(recall, precision)\n",
    "    except ValueError:\n",
    "        print(\"AUC-PR computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_pr = np.nan\n",
    "    return auc_pr\n",
    "\n",
    "\n",
    "def compute_auc_roc(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-ROC for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(y_true, anomaly_scores)\n",
    "    except ValueError:\n",
    "        print(\"AUC-ROC computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_roc = np.nan\n",
    "    return auc_roc"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_windows(data, window_size: int, step_size: int = 1):\n",
    "    \"\"\"\n",
    "    Given a 2D array `data` of shape (N, features), create overlapping windows\n",
    "    of shape (window_size, features). Returns array of shape (M, window_size, features).\n",
    "    If data is shorter than window_size, returns None.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    if N < window_size:\n",
    "        return None\n",
    "    windows = []\n",
    "    for i in range(0, N - window_size + 1, step_size):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return np.stack(windows, axis=0)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "class UCR(Dataset):\n",
    "    def __init__(self, path: str, window_size: int, step_size: int = 1, train: bool = True, data_id: int = 0):\n",
    "        self.path = path\n",
    "        self.window_size, self.step_size = window_size, step_size\n",
    "        if train:\n",
    "            self.data = self.create_windows('train', data_id)\n",
    "            self.labels = torch.zeros_like(self.data)\n",
    "        else:\n",
    "            self.data = self.create_windows('test', data_id)\n",
    "            self.labels = self.create_windows('labels', data_id)\n",
    "\n",
    "    def create_windows(self, tag: str, data_id: int):\n",
    "        files = [f for f in os.listdir(self.path) if f.endswith(f'{tag}.npy')]\n",
    "        windows = []\n",
    "        for file_path in files:\n",
    "            if data_id != 0 and not file_path.startswith(f'{str(data_id)}_'):\n",
    "                continue\n",
    "            array = np.load(os.path.join(self.path, file_path))\n",
    "            for i in range(0, len(array) - self.window_size + 1, self.step_size):\n",
    "                windows.append(array[i:i + self.window_size])\n",
    "        windows = np.array(windows) # because it's faster\n",
    "        return torch.tensor(windows, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "        return self.data[idx]\n",
    "\n",
    "def get_dataloaders(path: str, window_size: int, batch_size: int, step_size: int = 1, data_id: int = 0, shuffle: bool = False, seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    # Create datasets\n",
    "    train_dataset = UCR(path, window_size, step_size=step_size, train=True, data_id=data_id)\n",
    "    test_dataset = UCR(path, window_size, step_size=1, train=False, data_id=data_id) # test step size should always be 1\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :].to(x.device)"
   ]
  }
 ]
}
