{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyNvxU3vf4X/mw0SuqoiSqjy"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Packages",
   "metadata": {
    "id": "fB3U8aNVoWTs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install numpy tensorflow scikit-learn plotly pandas\n",
    "\n",
    "# For the UCR dataset we clone the git repo (if in Colab/Kaggle env)\n",
    "!git clone https://github.com/iMohammad97/anomaly_detection\n",
    "\n",
    "!pip install kaleido"
   ],
   "metadata": {
    "id": "jgIUvRdGoWI2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-12T13:23:15.830982Z",
     "iopub.execute_input": "2025-02-12T13:23:15.831286Z",
     "iopub.status.idle": "2025-02-12T13:23:34.646581Z",
     "shell.execute_reply.started": "2025-02-12T13:23:15.831264Z",
     "shell.execute_reply": "2025-02-12T13:23:34.645691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'anomaly_detection'...\nremote: Enumerating objects: 3828, done.\u001B[K\nremote: Counting objects: 100% (30/30), done.\u001B[K\nremote: Compressing objects: 100% (26/26), done.\u001B[K\nremote: Total 3828 (delta 16), reused 8 (delta 4), pack-reused 3798 (from 1)\u001B[K\nReceiving objects: 100% (3828/3828), 202.52 MiB | 33.48 MiB/s, done.\nResolving deltas: 100% (1623/1623), done.\nUpdating files: 100% (2721/2721), done.\nCollecting kaleido\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\nDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.9/79.9 MB\u001B[0m \u001B[31m21.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: kaleido\nSuccessfully installed kaleido-0.2.1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score, precision_score\n",
    "import glob, os, sys\n",
    "import kaleido\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import shutil\n",
    "import math"
   ],
   "metadata": {
    "id": "QlyJs93voTeo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739036175812,
     "user_tz": -210,
     "elapsed": 10718,
     "user": {
      "displayName": "Mohammad Mohammadi",
      "userId": "06409477078097051085"
     }
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-12T13:23:34.647871Z",
     "iopub.execute_input": "2025-02-12T13:23:34.648202Z",
     "iopub.status.idle": "2025-02-12T13:23:53.607582Z",
     "shell.execute_reply.started": "2025-02-12T13:23:34.648154Z",
     "shell.execute_reply": "2025-02-12T13:23:53.606669Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metrics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def pointwise_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Timepoint-wise precision: fraction of detected anomalies that are correct.\n",
    "    \"\"\"\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "def make_event(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Converts binary sequences (y_true and y_pred) into a list of (start, end) event tuples.\n",
    "    \"\"\"\n",
    "    y_true_starts = np.argwhere(np.diff(y_true.flatten(), prepend=0) == 1).flatten()\n",
    "    y_true_ends = np.argwhere(np.diff(y_true.flatten(), append=0) == -1).flatten()\n",
    "    y_true_events = list(zip(y_true_starts, y_true_ends))\n",
    "\n",
    "    y_pred_starts = np.argwhere(np.diff(y_pred, prepend=0) == 1).flatten()\n",
    "    y_pred_ends = np.argwhere(np.diff(y_pred, append=0) == -1).flatten()\n",
    "    y_pred_events = list(zip(y_pred_starts, y_pred_ends))\n",
    "\n",
    "    return y_true_events, y_pred_events\n",
    "\n",
    "\n",
    "def event_wise_recall(y_true_events, y_pred_events):\n",
    "    \"\"\"\n",
    "    Event-based recall. We consider an event 'detected' if the predicted event\n",
    "    overlaps with the true event in any way.\n",
    "    \"\"\"\n",
    "    detected_events = 0\n",
    "    for true_event in y_true_events:\n",
    "        true_start, true_end = true_event\n",
    "        for pred_event in y_pred_events:\n",
    "            pred_start, pred_end = pred_event\n",
    "            if pred_end >= true_start and pred_start <= true_end:\n",
    "                detected_events += 1\n",
    "                break\n",
    "    return detected_events / len(y_true_events) if y_true_events else 0\n",
    "\n",
    "\n",
    "def composite_f_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Combines timepoint precision and event-wise recall into a single F-score.\n",
    "    \"\"\"\n",
    "    prt = pointwise_precision(y_true, y_pred)\n",
    "    y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "    rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "    if prt + rece == 0:\n",
    "        return 0\n",
    "    return 2 * (prt * rece) / (prt + rece)\n",
    "\n",
    "\n",
    "def custom_auc_with_perfect_point(y_true, anomaly_scores, threshold_steps=100, plot=False):\n",
    "    \"\"\"\n",
    "    Generates thresholds, computes precision (timepoint) and recall (event-wise)\n",
    "    pairs, checks for a perfect point, and computes the AUC (area under the curve)\n",
    "    on the PR plane.\n",
    "    \"\"\"\n",
    "    percentiles = np.linspace(np.min(anomaly_scores), np.max(anomaly_scores) + 1e-7, threshold_steps)\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    perfect_point_found = False\n",
    "\n",
    "    for threshold in percentiles:\n",
    "        y_pred = (anomaly_scores >= threshold).astype(int)\n",
    "        prt = pointwise_precision(y_true, y_pred)\n",
    "\n",
    "        y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "        rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "\n",
    "        precision_list.append(prt)\n",
    "        recall_list.append(rece)\n",
    "\n",
    "        if prt == 1 and rece == 1:\n",
    "            perfect_point_found = True\n",
    "            break\n",
    "\n",
    "    # Compute AUC (precision vs recall)\n",
    "    custom_area = auc(recall_list, precision_list)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_list, precision_list, marker='o', label=f\"AUC = {custom_area:.4f}\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    return custom_area, perfect_point_found\n",
    "\n",
    "\n",
    "def compute_auc_pr(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-PR for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, anomaly_scores)\n",
    "        auc_pr = auc(recall, precision)\n",
    "    except ValueError:\n",
    "        print(\"AUC-PR computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_pr = np.nan\n",
    "    return auc_pr\n",
    "\n",
    "\n",
    "def compute_auc_roc(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-ROC for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(y_true, anomaly_scores)\n",
    "    except ValueError:\n",
    "        print(\"AUC-ROC computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_roc = np.nan\n",
    "    return auc_roc"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_windows(data, window_size: int, step_size: int = 1):\n",
    "    \"\"\"\n",
    "    Given a 2D array `data` of shape (N, features), create overlapping windows\n",
    "    of shape (window_size, features). Returns array of shape (M, window_size, features).\n",
    "    If data is shorter than window_size, returns None.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    if N < window_size:\n",
    "        return None\n",
    "    windows = []\n",
    "    for i in range(0, N - window_size + 1, step_size):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return np.stack(windows, axis=0)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "class UCR(Dataset):\n",
    "    def __init__(self, path: str, window_size: int, step_size: int = 1, train: bool = True, data_id: int = 0):\n",
    "        self.path = path\n",
    "        self.window_size, self.step_size = window_size, step_size\n",
    "        if train:\n",
    "            self.data = self.create_windows('train', data_id)\n",
    "            self.labels = torch.zeros_like(self.data)\n",
    "        else:\n",
    "            self.data = self.create_windows('test', data_id)\n",
    "            self.labels = self.create_windows('labels', data_id)\n",
    "\n",
    "    def create_windows(self, tag: str, data_id: int):\n",
    "        files = [f for f in os.listdir(self.path) if f.endswith(f'{tag}.npy')]\n",
    "        windows = []\n",
    "        for file_path in files:\n",
    "            if data_id != 0 and not file_path.startswith(f'{str(data_id)}_'):\n",
    "                continue\n",
    "            array = np.load(os.path.join(self.path, file_path))\n",
    "            for i in range(0, len(array) - self.window_size + 1, self.step_size):\n",
    "                windows.append(array[i:i + self.window_size])\n",
    "        windows = np.array(windows) # because it's faster\n",
    "        return torch.tensor(windows, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "        return self.data[idx]\n",
    "\n",
    "def get_dataloaders(path: str, window_size: int, batch_size: int, step_size: int = 1, data_id: int = 0, shuffle: bool = False, seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    # Create datasets\n",
    "    train_dataset = UCR(path, window_size, step_size=step_size, train=True, data_id=data_id)\n",
    "    test_dataset = UCR(path, window_size, step_size=1, train=False, data_id=data_id) # test step size should always be 1\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :].to(x.device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM AutoEncoder"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMAutoencoder:\n",
    "    def __init__(self,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 labels,\n",
    "                 timesteps: int = 128,\n",
    "                 features: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 lstm_units: int = 64,\n",
    "                 step_size: int = 1,\n",
    "                 threshold_sigma=2.0,\n",
    "                 seed: int = 0):\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.labels = labels\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.step_size = step_size\n",
    "        self.threshold_sigma = threshold_sigma\n",
    "\n",
    "        # Prepare windowed data\n",
    "        self.train_data_window = create_windows(self.train_data, timesteps, step_size)\n",
    "        self.test_data_window = create_windows(self.test_data, timesteps, 1)\n",
    "\n",
    "        # Model placeholders\n",
    "        self.model = None\n",
    "        self.threshold = 0\n",
    "\n",
    "        # Arrays to hold predictions\n",
    "        if self.test_data_window is not None:\n",
    "            self.predictions_windows = np.zeros(len(self.test_data_window))\n",
    "        self.anomaly_preds = np.zeros(len(self.test_data))\n",
    "        self.anomaly_errors = np.zeros(len(self.test_data))\n",
    "        self.predictions = np.zeros(len(self.test_data))\n",
    "\n",
    "        self.losses = {'train': [], 'valid': []}\n",
    "        self.name = 'LSTMAutoencoder'  # A name attribute for the class\n",
    "\n",
    "        set_seed(seed)\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = tf.keras.Input(shape=(self.timesteps, self.features), name='input_layer')\n",
    "\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True, name='lstm_1')(inputs)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=False, name='latent')(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.timesteps, name='repeat_vector')(x)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=True, name='lstm_3')(x)\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True, name='lstm_4')(x)\n",
    "        outputs = layers.TimeDistributed(layers.Dense(self.features, name='dense_output'))(x)\n",
    "\n",
    "        self.model = models.Model(inputs, outputs, name='model')\n",
    "\n",
    "    def compute_threshold(self):\n",
    "        rec = self.model.predict(self.train_data_window, verbose=0)\n",
    "        mse = np.mean(np.square(self.train_data_window - rec), axis=(1, 2))\n",
    "        self.threshold = np.mean(mse) + self.threshold_sigma * np.std(mse)\n",
    "\n",
    "    def train(self,\n",
    "              batch_size=32,\n",
    "              epochs=50,\n",
    "              optimizer='adam',\n",
    "              loss='mse',\n",
    "              patience=10,\n",
    "              shuffle: bool = False,\n",
    "              seed: int = 42):\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Custom max-diff loss function\n",
    "        def max_diff_loss(y_true, y_pred):\n",
    "            return tf.reduce_max(tf.abs(y_true - y_pred), axis=[1, 2])\n",
    "\n",
    "        # Determine which loss function to use\n",
    "        loss_function = 'mse' if loss == 'mse' else max_diff_loss\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            self.train_data_window, self.train_data_window,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            validation_split=0.1,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        self.losses['train'] = [float(l) for l in history.history['loss']]\n",
    "        self.losses['valid'] = [float(l) for l in history.history['val_loss']]\n",
    "\n",
    "    def evaluate(self, batch_size=32, loss='mse'):\n",
    "        \"\"\"\n",
    "        Evaluate the model on self.test_data_window.\n",
    "        Sets self.anomaly_preds, self.anomaly_errors, self.predictions.\n",
    "        \"\"\"\n",
    "        if self.test_data_window is None or len(self.test_data_window) == 0:\n",
    "            print(\"No test windows available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        length = self.test_data.shape[0]\n",
    "        self.compute_threshold()\n",
    "\n",
    "        # Generate predictions for the test data windows\n",
    "        self.predictions_windows = self.model.predict(self.test_data_window, batch_size=batch_size)\n",
    "\n",
    "        # Compute reconstruction errors\n",
    "        if loss == 'mse':\n",
    "            errors = np.mean(np.square(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "        else:\n",
    "            # If using max_diff_loss\n",
    "            errors = np.max(np.abs(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "\n",
    "        # Expand window errors to match original time steps\n",
    "        M = errors.shape[0]\n",
    "        timestep_errors = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            timestep_errors[start:end + 1] += errors[i]\n",
    "            counts[start:end + 1] += 1\n",
    "\n",
    "        counts[counts == 0] = 1  # Avoid division by zero\n",
    "        timestep_errors /= counts  # Average overlapping windows\n",
    "\n",
    "        self.anomaly_preds = (timestep_errors > self.threshold).astype(int)\n",
    "        self.anomaly_errors = timestep_errors\n",
    "\n",
    "        # Compute predictions (averaged across windows)\n",
    "        counts = np.zeros(length)\n",
    "        self.predictions = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                timestep_index = i + j\n",
    "                if timestep_index < length:\n",
    "                    self.predictions[timestep_index] += self.predictions_windows[i, j]\n",
    "                    counts[timestep_index] += 1\n",
    "\n",
    "        # Avoid division by zero\n",
    "        for i in range(length):\n",
    "            if counts[i] > 0:\n",
    "                self.predictions[i] /= counts[i]\n",
    "\n",
    "        self.predictions = np.nan_to_num(self.predictions)\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        \"\"\"\n",
    "        Returns latent representation from the encoder part of the model.\n",
    "        \"\"\"\n",
    "        encoder_model = models.Model(inputs=self.model.input,\n",
    "                                    outputs=self.model.get_layer('latent').output)\n",
    "        latent_representations = encoder_model.predict(x)\n",
    "        return latent_representations\n",
    "\n",
    "    def save_model(self, model_path: str = \"model.h5\"):\n",
    "        \"\"\"\n",
    "        Save the Keras model to disk.\n",
    "        \"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            print(\"No model to save.\")\n",
    "\n",
    "    def load_model(self, model_path: str, train_path: str, test_path: str, label_path: str):\n",
    "        \"\"\"\n",
    "        Load the Keras model from the specified file paths and set\n",
    "        self.train_data, self.test_data, and self.labels accordingly.\n",
    "        \"\"\"\n",
    "        self.model = models.load_model(model_path, compile=False)\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "        # Load data\n",
    "        self.train_data = np.load(train_path)\n",
    "        self.test_data = np.load(test_path)\n",
    "        self.labels = np.load(label_path)\n",
    "\n",
    "        # Recreate windows\n",
    "        self.train_data_window = create_windows(self.train_data, self.timesteps)\n",
    "        self.test_data_window = create_windows(self.test_data, self.timesteps)\n",
    "\n",
    "        print(f\"Loaded model from {model_path} and data from {train_path}, {test_path}, {label_path}.\")\n",
    "\n",
    "    def plot_results(self, save_path=None, file_format='html',size=800):\n",
    "        \"\"\"\n",
    "        Plot test data, predictions, anomaly errors, and highlight\n",
    "        labeled anomalies and predicted anomalies.\n",
    "        \"\"\"\n",
    "        # Flatten arrays\n",
    "        test_data = self.test_data.ravel()\n",
    "        anomaly_preds = self.anomaly_preds\n",
    "        anomaly_errors = self.anomaly_errors\n",
    "        predictions = self.predictions\n",
    "        labels = self.labels.ravel()\n",
    "\n",
    "        if not (len(test_data) == len(labels) == len(anomaly_preds) == len(anomaly_errors) == len(predictions)):\n",
    "            raise ValueError(\"All input arrays must have the same length.\")\n",
    "\n",
    "        plot_width = max(size, len(test_data) // 10)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        # Test Data\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        # Predictions\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(predictions))),\n",
    "                                 y=predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        # Anomaly Errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(anomaly_errors))),\n",
    "                                 y=anomaly_errors,\n",
    "                                 mode='lines',\n",
    "                                 name='Anomaly Errors',\n",
    "                                 line=dict(color='red')))\n",
    "\n",
    "        # Labeled anomalies\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels on Test Data',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Predicted anomalies\n",
    "        anomaly_pred_indices = [i for i in range(len(anomaly_preds)) if anomaly_preds[i] == 1]\n",
    "        if anomaly_pred_indices:\n",
    "            fig.add_trace(go.Scatter(x=anomaly_pred_indices,\n",
    "                                     y=[predictions[i] for i in anomaly_pred_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Anomaly Predictions',\n",
    "                                     marker=dict(color='green', size=10)))\n",
    "\n",
    "        fig.update_layout(title='Test Data, Predictions, and Anomalies',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "\n",
    "        # Optionally save the figure\n",
    "        if save_path is not None:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "            if file_format.lower() == 'html':\n",
    "                # Save as interactive HTML\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                # Save as static image (requires kaleido or orca)\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    def plot_losses(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot training and validation losses.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses['train'], label='Training Loss')\n",
    "        plt.plot(self.losses['valid'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "            # Save the figure\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM SAE"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "class StationaryLoss(layers.Layer):\n",
    "    def call(self, latent, mean_coef: float = 1.0, std_coef: float = 1.0):\n",
    "        # Calculate the average of the latent space\n",
    "        latent_avg = tf.reduce_mean(latent, axis=0)\n",
    "        mse_loss = tf.reduce_mean(tf.abs(latent_avg))\n",
    "        self.add_loss(mean_coef * mse_loss)\n",
    "        \n",
    "        # Calculate the standard deviation of the latent space\n",
    "        latent_std = tf.math.reduce_std(latent, axis=0)\n",
    "        std_loss = tf.reduce_mean(tf.abs(latent_std - 1.0))\n",
    "        self.add_loss(std_coef * std_loss)\n",
    "        \n",
    "        # Store the losses separately for logging\n",
    "        self.mse_loss = mean_coef * mse_loss\n",
    "        self.std_loss = std_coef * std_loss\n",
    "        \n",
    "        return latent\n",
    "\n",
    "\n",
    "class StationaryLSTMAutoencoder:\n",
    "    def __init__(self, train_data, test_data, labels, timesteps: int = 128, features: int = 1, latent_dim: int = 32, lstm_units: int = 64, step_size: int = 1, threshold_sigma=2.0, seed: int = 0):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.train_data_window = create_windows(self.train_data, timesteps, step_size)\n",
    "        self.test_data_window = create_windows(self.test_data, timesteps, 1)\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = None  # Model is not built yet.\n",
    "        self.threshold_sigma = threshold_sigma\n",
    "        self.threshold = 0\n",
    "        self.predictions_windows = np.zeros(len(self.test_data_window))\n",
    "        self.anomaly_preds = np.zeros(len(self.test_data))\n",
    "        self.anomaly_errors = np.zeros(len(self.test_data))\n",
    "        self.predictions = np.zeros(len(self.test_data))\n",
    "        self.labels = labels\n",
    "        self.name = \"LSTM_SAE\"\n",
    "        self.losses = {\"mse\": [], \"mean\": [], \"std\": []}\n",
    "        set_seed(seed)\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Encoder\n",
    "        inputs = tf.keras.Input(shape=(self.timesteps, self.features))\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True)(inputs)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=False)(x)\n",
    "        latent = layers.Dense(self.latent_dim)(x)\n",
    "\n",
    "        # Apply custom loss to the latent space\n",
    "        latent_with_loss = StationaryLoss()(latent, mean_coef=1.0, std_coef=1.0)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.timesteps)(latent_with_loss)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=True)(x)\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True)(x)\n",
    "        outputs = layers.TimeDistributed(layers.Dense(self.features))(x)\n",
    "\n",
    "        # DAE Model\n",
    "        self.model = models.Model(inputs, outputs)  # Return only the outputs (no KL divergence in this case)\n",
    "\n",
    "    def train(self, batch_size: int = 32, epochs: int = 50, optimizer: str = 'adam', patience: int = 15, seed: int = 42, shuffle: bool = False):\n",
    "        set_seed(seed)\n",
    "        # Ensure the optimizer is set up correctly\n",
    "        if isinstance(optimizer, str):\n",
    "            optimizer = tf.keras.optimizers.get(optimizer)  # Get optimizer by name\n",
    "        elif not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n",
    "            raise ValueError(\"Optimizer must be a string or a tf.keras.optimizers.Optimizer instance.\")\n",
    "\n",
    "        # Loss function\n",
    "        mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Track losses\n",
    "        mse_loss_tracker = tf.keras.metrics.Mean(name=\"mse_loss\")\n",
    "        mean_loss_tracker = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "        std_loss_tracker = tf.keras.metrics.Mean(name=\"std_loss\")\n",
    "\n",
    "        # Early stopping variables \n",
    "        best_epoch_loss = float('inf') \n",
    "        patience_counter = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            mse_loss_tracker.reset_state()\n",
    "            mean_loss_tracker.reset_state()\n",
    "            std_loss_tracker.reset_state()\n",
    "\n",
    "            if shuffle:\n",
    "                np.random.shuffle(self.train_data_window)\n",
    "\n",
    "            for step in trange(0, len(self.train_data_window), batch_size, leave=False):\n",
    "                batch_data = self.train_data_window[step:step + batch_size]\n",
    "                epoch_loss = 0 # Should be changed to val loss later\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    reconstructed = self.model(batch_data, training=True)\n",
    "\n",
    "                    # Compute reconstruction loss\n",
    "                    mse_loss = mse_loss_fn(batch_data, reconstructed)\n",
    "\n",
    "                    # Get custom losses from the model\n",
    "                    mean_loss = tf.reduce_mean([layer.mse_loss for layer in self.model.layers if isinstance(layer, StationaryLoss)])\n",
    "                    std_loss = tf.reduce_mean([layer.std_loss for layer in self.model.layers if isinstance(layer, StationaryLoss)])\n",
    "\n",
    "                    # Total loss\n",
    "                    total_loss = mse_loss + mean_loss + std_loss\n",
    "\n",
    "                # Compute gradients and update weights\n",
    "                gradients = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "\n",
    "                # Track losses\n",
    "                mse_loss_tracker.update_state(mse_loss)\n",
    "                mean_loss_tracker.update_state(mean_loss)\n",
    "                std_loss_tracker.update_state(std_loss)\n",
    "\n",
    "                epoch_loss += total_loss\n",
    "\n",
    "            # Log losses after each epoch\n",
    "            self.losses['mse'].append(float(mse_loss_tracker.result().numpy()))\n",
    "            self.losses['mean'].append(float(mean_loss_tracker.result().numpy()))\n",
    "            self.losses['std'].append(float(std_loss_tracker.result().numpy()))\n",
    "            pbar.set_description(\n",
    "                f\"MSE Loss = {self.losses['mse'][-1]:.4f}, Mean Loss = {self.losses['mean'][-1]:.4f}, STD Loss = {self.losses['std'][-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping logic \n",
    "            if epoch_loss < best_epoch_loss: \n",
    "                best_epoch_loss = epoch_loss \n",
    "                patience_counter = 0  \n",
    "            else: \n",
    "                patience_counter += 1 \n",
    "                if patience_counter >= patience: \n",
    "                    print(f\"Early stopping triggered after {epoch + 1} epochs.\") \n",
    "                    break\n",
    "\n",
    "    def compute_threshold(self):\n",
    "        rec = self.model.predict(self.train_data_window, verbose=0)\n",
    "        mse = np.mean(np.square(self.train_data_window - rec), axis=(1, 2))\n",
    "        self.threshold = np.mean(mse) + self.threshold_sigma * np.std(mse)\n",
    "\n",
    "    def evaluate(self, batch_size=32):\n",
    "        length = self.test_data.shape[0]\n",
    "        self.compute_threshold()\n",
    "        # Generate predictions for the test data windows\n",
    "        self.predictions_windows = self.model.predict(self.test_data_window, batch_size=batch_size)\n",
    "        mse = np.mean(np.square(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "\n",
    "        # Expand errors to original length\n",
    "        M = mse.shape[0]\n",
    "        timestep_errors = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "\n",
    "        # Each window i covers timesteps [i, i+window_size-1]\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            timestep_errors[start:end + 1] += mse[i]\n",
    "            counts[start:end + 1] += 1\n",
    "\n",
    "        counts[counts == 0] = 1  # Avoid division by zero\n",
    "        timestep_errors /= counts  # Average overlapping windows\n",
    "\n",
    "        # Generate anomaly predictions based on the threshold\n",
    "        self.anomaly_preds = (timestep_errors > self.threshold).astype(int)\n",
    "        self.anomaly_errors = timestep_errors\n",
    "\n",
    "        counts = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                timestep_index = i + j  # This is the index in the timestep corresponding to the current prediction\n",
    "                if timestep_index < length:  # Ensure we don't go out of bounds\n",
    "                    self.predictions[timestep_index] += self.predictions_windows[i, j]  # Accumulate each prediction appropriately\n",
    "                    counts[timestep_index] += 1\n",
    "\n",
    "        # Divide by counts to get the average prediction\n",
    "        for i in range(length):\n",
    "            if counts[i] > 0:\n",
    "                self.predictions[i] /= counts[i]\n",
    "\n",
    "        self.predictions = np.nan_to_num(self.predictions)\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        encoder_model = models.Model(inputs=self.model.input, outputs=self.model.get_layer('latent').output)\n",
    "        latent_representations = encoder_model.predict(x)\n",
    "        return latent_representations\n",
    "\n",
    "\n",
    "    def save_model(self, model_path: str = \"model.h5\"):\n",
    "        \n",
    "        # Save the Keras model\n",
    "        if self.model is not None:\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            print(\"No model to save.\")\n",
    "\n",
    "    \n",
    "    def load_model(self, model_path: str, train_path: str, test_path: str, label_path: str):\n",
    "        # Use custom_object_scope for the custom layer\n",
    "        with custom_object_scope({'StationaryLoss': StationaryLoss}):\n",
    "            self.model = models.load_model(\n",
    "                model_path,\n",
    "                compile=False  # Avoid recompiling until the model is fully loaded\n",
    "            )\n",
    "    \n",
    "        # Compile the model for evaluation or retraining\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error']\n",
    "        )\n",
    "    \n",
    "        # Load data\n",
    "        self.train_data = np.load(train_path)\n",
    "        self.test_data = np.load(test_path)\n",
    "        self.labels = np.load(label_path)\n",
    "    \n",
    "        # Recreate the windows with the newly loaded data\n",
    "        self.train_data_window = create_windows(self.train_data, self.timesteps)\n",
    "        self.test_data_window = create_windows(self.test_data, self.timesteps)\n",
    "    \n",
    "    def plot_results(self, size=800, save_path=None, file_format='html'):\n",
    "        # Flattening arrays to ensure they are 1D\n",
    "        test_data = self.test_data.ravel()  # Convert to 1D array\n",
    "        anomaly_preds = self.anomaly_preds  # Already 1D\n",
    "        anomaly_errors = self.anomaly_errors  # Already 1D\n",
    "        predictions = self.predictions  # Already 1D\n",
    "        labels = self.labels.ravel()  # Convert to 1D array\n",
    "\n",
    "        plot_width = max(size, len(test_data) // 10)  # Ensure a minimum width of 800, scale with data length\n",
    "\n",
    "        # Check if all inputs have the same length\n",
    "        if not (len(test_data) == len(labels) == len(anomaly_preds) == len(anomaly_errors) == len(predictions)):\n",
    "            raise ValueError(\"All input arrays must have the same length.\")\n",
    "\n",
    "        # Create a figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add traces for test data, predictions, and anomaly errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(predictions))),\n",
    "                                 y=predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Predictions'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(anomaly_errors))),\n",
    "                                 y=anomaly_errors,\n",
    "                                 mode='lines',\n",
    "                                 name='Anomaly Errors'))\n",
    "\n",
    "        # Highlight points in test_data where label is 1\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels on Test Data',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Set the layout\n",
    "        fig.update_layout(title='Test Data, Predictions, and Anomalies',\n",
    "                          xaxis_title='Time Steps', yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    "                          template='plotly', width=plot_width)\n",
    "        # Optionally save the figure\n",
    "        if save_path is not None:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "            if file_format.lower() == 'html':\n",
    "                # Save as interactive HTML\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                # Save as static image (requires kaleido or orca)\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "    def plot_losses(self, save_path=None):\n",
    "        # Plot the loss values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses['mse'], label='MSE Reconstruction Loss')\n",
    "        plt.plot(self.losses['mean'], label='Latent Mean Loss')\n",
    "        plt.plot(self.losses['std'], label='Latent Standard Deviation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if save_path:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "            # Save the figure\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer Autoencoder"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerAE(nn.Module):\n",
    "    def __init__(self, n_features: int = 1, window_size: int = 256, d_model: int = 64, nhead: int = 8, num_layers: int = 3, dim_feedforward: int = 256, dropout: float = 0.1, device: str = 'cpu', seed: int = 0):\n",
    "        super(TransformerAE, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.name = 'TransformerAE'\n",
    "        self.lr = 0.0001\n",
    "        self.device = device\n",
    "        self.n_features = n_features\n",
    "        self.window_size = window_size\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.input_projection = nn.Linear(n_features, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=window_size)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.output_projection = nn.Linear(d_model, n_features)\n",
    "\n",
    "        # Learnable start token for decoder input\n",
    "        self.start_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.to(device)\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        self.losses = []\n",
    "\n",
    "        self.threshold = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Input projection and positional encoding\n",
    "        x = self.input_projection(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Encoder\n",
    "        memory = self.encoder(x)\n",
    "\n",
    "        # Decoder input: start token followed by zeros\n",
    "        start_token = self.start_token.expand(1, batch_size, -1)  # (1, batch_size, d_model)\n",
    "        tgt = torch.zeros_like(x).to(x.device)  # (seq_len, batch_size, d_model)\n",
    "        tgt[0] = start_token  # Insert start token at the beginning\n",
    "\n",
    "        # Decoder\n",
    "        output = self.decoder(tgt, memory)\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.output_projection(output)\n",
    "        return output\n",
    "\n",
    "    def select_loss(self, loss_name: str):\n",
    "        if loss_name == \"MSE\":\n",
    "            return nn.MSELoss(reduction='mean').to(self.device)\n",
    "        elif loss_name == \"Huber\":\n",
    "            return nn.SmoothL1Loss(reduction='mean').to(self.device)\n",
    "        elif loss_name == \"MaxDiff\":\n",
    "            return lambda inputs, target: torch.max(torch.abs(inputs - target))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    def learn(self, train_loader, n_epochs: int, loss_name: str = \"MaxDiff\", seed: int = 42):\n",
    "        torch.manual_seed(seed)\n",
    "        self.train()\n",
    "        loss_fn = self.select_loss(loss_name)\n",
    "        for _ in (pbar := trange(n_epochs)):\n",
    "            recons = []\n",
    "            for d, a in (p := tqdm(train_loader, leave=False)):\n",
    "                d = d.to(self.device)\n",
    "                x = self.forward(d)\n",
    "                loss = loss_fn(x, d)\n",
    "                recons.append(loss.item())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                p.set_description(f'Batch Loss = {recons[-1]:.4f}')\n",
    "            pbar.set_description(f'{loss_name} Loss = {np.mean(recons):.4f}')\n",
    "            self.losses.append(np.mean(recons))\n",
    "\n",
    "    def predict(self, data, train: bool = False):\n",
    "        self.eval()\n",
    "        results = {}\n",
    "        inputs, anomalies, outputs, errors = [], [], [], []\n",
    "        loss = nn.MSELoss(reduction='none').to(self.device)\n",
    "        with torch.no_grad():\n",
    "            for window, anomaly in data:\n",
    "                if window.shape[0] == 1:\n",
    "                    break\n",
    "                inputs.append(window.squeeze().T[-1])\n",
    "                anomalies.append(anomaly.squeeze().T[-1])\n",
    "                window = window.to(self.device)\n",
    "                recons = self.forward(window)\n",
    "                outputs.append(recons.cpu().detach().numpy().squeeze().T[-1])\n",
    "                errors.append(loss(window, recons).cpu().detach().numpy().squeeze().T[-1])\n",
    "        results['inputs'] = np.concatenate(inputs)\n",
    "        results['anomalies'] = np.concatenate(anomalies)\n",
    "        results['outputs'] = np.concatenate(outputs)\n",
    "        results['errors'] = np.concatenate(errors)\n",
    "        if train:\n",
    "            self.threshold = np.mean(results['errors']) + 3 * np.std(results['errors'])\n",
    "        elif self.threshold:\n",
    "            results['predictions'] = [1 if error > self.threshold else 0 for error in results['errors']]\n",
    "        return results\n",
    "\n",
    "    def plot_results(self, data, train: bool = False, plot_width: int = 800, save_path=None, file_format='html'):\n",
    "        results = self.predict(data, train=train)\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(results['inputs']))),\n",
    "                                 y=results['inputs'],\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(results['outputs']))),\n",
    "                                 y=results['outputs'],\n",
    "                                 mode='lines',\n",
    "                                 name='Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(results['errors']))),\n",
    "                                 y=results['errors'],\n",
    "                                 mode='lines',\n",
    "                                 name='Anomaly Errors',\n",
    "                                 line=dict(color='red')))\n",
    "        label_indices = [i for i in range(len(results['anomalies'])) if results['anomalies'][i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[results['inputs'][i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels on Test Data',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "        if self.threshold is not None and not train:\n",
    "            label_indices = [i for i in range(len(results['anomalies'])) if results['predictions'][i] == 1]\n",
    "            fig.add_hline(y=self.threshold, name='Threshold')\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[results['inputs'][i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Predictions on Test Data',\n",
    "                                     marker=dict(color='black', size=7, symbol='x')))\n",
    "        fig.update_layout(title='Test Data, Predictions, and Anomalies',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "        # Optionally save the figure\n",
    "        if save_path is not None:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "            if file_format.lower() == 'html':\n",
    "                # Save as interactive HTML\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                # Save as static image (requires kaleido or orca)\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "        fig.show()\n",
    "\n",
    "    def plot_losses(self, fig_size=(10, 6), save_path=None):\n",
    "        xs = np.arange(len(self.losses)) + 1\n",
    "        plt.figure(figsize=fig_size)\n",
    "        plt.plot(xs, self.losses, label='Total Loss')\n",
    "        plt.grid()\n",
    "        plt.xticks(xs)\n",
    "        plt.legend()\n",
    "        if save_path:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "            # Save the figure\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, path: str = ''):\n",
    "        \"\"\"\n",
    "        Save the model, optimizer state, and training history to a file.\n",
    "        \"\"\"\n",
    "        if path == '':\n",
    "            path = self.name + '_' + str(len(self.losses)).zfill(3) + '.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'losses': self.losses,\n",
    "            'config': {\n",
    "                'n_features': self.n_features,\n",
    "                'window_size': self.window_size,\n",
    "                'd_model': self.d_model,\n",
    "                'nhead': self.nhead,\n",
    "                'num_layers': self.num_layers,\n",
    "                'dim_feedforward': self.dim_feedforward,\n",
    "                'dropout': self.dropout,\n",
    "                'device': self.device,\n",
    "                'lr': self.lr,\n",
    "            }\n",
    "        }, path)\n",
    "        print(f'Model saved to path = {path}')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str):\n",
    "        checkpoint = torch.load(path)\n",
    "        config = checkpoint['config']\n",
    "        model = TransformerAE(\n",
    "            n_features=config['n_features'],\n",
    "            window_size=config['window_size'],\n",
    "            d_model=config['d_model'],\n",
    "            nhead=config['nhead'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dim_feedforward=config['dim_feedforward'],\n",
    "            dropout=config['dropout'],\n",
    "            device=config['device']\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.losses = checkpoint['losses']\n",
    "\n",
    "        return model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation Function"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###############################################################################\n",
    "# Universal Evaluation Function\n",
    "###############################################################################\n",
    "def evaluate_model_and_save_results(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    results_csv_path,\n",
    "    # For LSTMAutoencoder or StationaryLSTMAutoencoder:\n",
    "    train_path=None,\n",
    "    test_path=None,\n",
    "    label_path=None,\n",
    "    loss_type='mse',\n",
    "    # For TransformerAE (PyTorch):\n",
    "    test_loader=None,\n",
    "    train_data=None,\n",
    "    test_data=None,\n",
    "    labels=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates one of these classes:\n",
    "      - LSTMAutoencoder (Keras)\n",
    "      - StationaryLSTMAutoencoder (Keras)\n",
    "      - TransformerAE (PyTorch)\n",
    "\n",
    "    and saves results (AUC metrics, F1, etc.) to CSV.\n",
    "\n",
    "    :param model_class: The class to evaluate.\n",
    "                       Possible values: LSTMAutoencoder, StationaryLSTMAutoencoder, TransformerAE\n",
    "    :param model_path:  For Keras -> .h5 path\n",
    "                        For PyTorch -> .pth checkpoint\n",
    "    :param results_csv_path: Where to save the CSV of results.\n",
    "    :param train_path:   For Keras models: path to .npy file containing training data\n",
    "    :param test_path:    For Keras models: path to .npy file containing test data\n",
    "    :param label_path:   For Keras models: path to .npy file containing labels\n",
    "    :param loss_type:    'mse' or 'max_diff' for LSTMAutoencoder. \n",
    "                         (StationaryLSTMAutoencoder only supports MSE.)\n",
    "    :param test_loader:  PyTorch DataLoader for the test set (TransformerAE only).\n",
    "    \"\"\"\n",
    "    # CASE 1: LSTMAutoencoder (Keras-based)\n",
    "    if model_class.__name__ == \"LSTMAutoencoder\":\n",
    "        if not (train_path and test_path and label_path):\n",
    "            raise ValueError(\"For LSTMAutoencoder, provide train_path, test_path, label_path.\")\n",
    "\n",
    "        dummy_model = model_class(\n",
    "            train_data=np.array([]),\n",
    "            test_data=np.array([]),\n",
    "            labels=np.array([])\n",
    "        )\n",
    "        dummy_model.load_model(model_path, train_path, test_path, label_path)\n",
    "\n",
    "        # Evaluate using the specified loss_type\n",
    "        dummy_model.evaluate(loss=loss_type)\n",
    "\n",
    "        y_true = dummy_model.labels.flatten()\n",
    "        anomaly_scores = dummy_model.anomaly_errors\n",
    "        y_pred = dummy_model.anomaly_preds\n",
    "\n",
    "    # CASE 2: StationaryLSTMAutoencoder (Keras-based, MSE-only)\n",
    "    elif model_class.__name__ == \"StationaryLSTMAutoencoder\":\n",
    "        if not (train_path and test_path and label_path):\n",
    "            raise ValueError(\"For StationaryLSTMAutoencoder, provide train_path, test_path, label_path.\")\n",
    "\n",
    "        # dummy_model = model_class(\n",
    "        #     train_data=np.array([]),\n",
    "        #     test_data=np.array([]),\n",
    "        #     labels=np.array([])\n",
    "        # )\n",
    "        dummy_model = model_class(\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "            labels=labels,\n",
    "        )\n",
    "        # Typically expects the same load_model signature as LSTMAutoencoder:\n",
    "        dummy_model.load_model(model_path, train_path, test_path, label_path)\n",
    "\n",
    "        # StationaryLSTMAutoencoder only has MSE, so no need to pass loss_type\n",
    "        dummy_model.evaluate()\n",
    "\n",
    "        y_true = dummy_model.labels.flatten()\n",
    "        anomaly_scores = dummy_model.anomaly_errors\n",
    "        y_pred = dummy_model.anomaly_preds\n",
    "\n",
    "    # CASE 3: TransformerAE (PyTorch-based)\n",
    "    elif model_class.__name__ == \"TransformerAE\":\n",
    "        if test_loader is None:\n",
    "            raise ValueError(\"For TransformerAE, please provide test_loader for evaluation.\")\n",
    "\n",
    "        # Load your PyTorch model\n",
    "        model = model_class.load(model_path)\n",
    "\n",
    "        # Use .predict() to get inputs, anomalies, outputs, errors, predictions\n",
    "        results = model.predict(test_loader, train=False)\n",
    "\n",
    "        y_true = results['anomalies']\n",
    "        anomaly_scores = results['errors']\n",
    "\n",
    "        # If threshold was set internally, 'predictions' will be in results\n",
    "        if 'predictions' in results:\n",
    "            y_pred = np.array(results['predictions'])\n",
    "        else:\n",
    "            # Fallback threshold\n",
    "            threshold = np.mean(anomaly_scores) + 3.0 * np.std(anomaly_scores)\n",
    "            y_pred = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model class. \"\n",
    "                         \"Must be LSTMAutoencoder, StationaryLSTMAutoencoder, or TransformerAE.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute Metrics\n",
    "    # -------------------------------------------------------------------------\n",
    "    custom_auc_val, perfect_point_found = custom_auc_with_perfect_point(y_true, anomaly_scores)\n",
    "    auc_pr_val = compute_auc_pr(y_true, anomaly_scores)\n",
    "    auc_roc_val = compute_auc_roc(y_true, anomaly_scores)\n",
    "    composite_f1_val = composite_f_score(y_true, y_pred)\n",
    "\n",
    "    # Prepare results\n",
    "    results_dict = {\n",
    "        \"ModelPath\": model_path,\n",
    "        \"AUC_Custom\": custom_auc_val,\n",
    "        \"PerfectPointFound\": perfect_point_found,\n",
    "        \"AUC_PR\": auc_pr_val,\n",
    "        \"AUC_ROC\": auc_roc_val,\n",
    "        \"CompositeF1\": composite_f1_val\n",
    "    }\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([results_dict])\n",
    "    df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"Results saved to {results_csv_path}\")\n",
    "    print(\"RESULTS:\")\n",
    "    print(df)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Usage"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM Autoencoder Usage"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "run_list = [\"2\", \"37\", \"45\", \"23\", \"43\", \"21\", \"4\", \"8\", \"11\", \"20\", \"50\", \"51\", \"69\", \"31\", \"6\"]\n",
    "\n",
    "# Specify the directory you want to search in\n",
    "directory = \"/kaggle/working/anomaly_detection/UCR/UCR2_preprocessed/\"\n",
    "\n",
    "# Use glob to find all folders within the specified directory\n",
    "files = glob.glob(f\"{directory}/*/\")\n",
    "\n",
    "# Hyper-parameters\n",
    "HYPER_PARAMS = {\n",
    "    \"TIMESTEPS\": 128,\n",
    "    \"LATENT_DIM\": 32,\n",
    "    \"STEP_SIZE\": 10,\n",
    "    \"EPOCHS\": 200\n",
    "}\n",
    "\n",
    "# Make directory for results\n",
    "!rm -rf results\n",
    "!mkdir results\n",
    "\n",
    "# Gather all the time-series in the category\n",
    "train_files = sorted(glob.glob(os.path.join(directory, \"*_train.npy\")))\n",
    "\n",
    "# For each time-serie in a category train the model\n",
    "for ts in train_files:\n",
    "    # break # COMMENT THIS IF YOU WANT TO TRAIN LSTM\n",
    "    ts_id = ts.split(\"/\")[-1].split(\"_\")[0]\n",
    "    if not ts_id in run_list:\n",
    "            continue\n",
    "    print(f\"starting w/ ts {ts_id}\")\n",
    "    \n",
    "    # Find test and labels file for the time-serie\n",
    "    train_file = ts\n",
    "    test_file = ts.replace(\"_train.npy\", \"_test.npy\")\n",
    "    label_file = ts.replace(\"_train.npy\", \"_labels.npy\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train = np.load(train_file)\n",
    "    X_test = np.load(test_file)\n",
    "    Y_test = np.load(label_file)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = LSTMAutoencoder(train_data=X_train,\n",
    "                        test_data=X_test,\n",
    "                        labels=Y_test,\n",
    "                        timesteps=HYPER_PARAMS[\"TIMESTEPS\"],\n",
    "                        latent_dim=HYPER_PARAMS[\"LATENT_DIM\"],\n",
    "                        step_size=HYPER_PARAMS[\"STEP_SIZE\"])\n",
    "    \n",
    "    # Train\n",
    "    model.train(epochs=HYPER_PARAMS[\"EPOCHS\"], batch_size=32, patience=30, loss='max_diff_loss')\n",
    "\n",
    "    # Evaluate\n",
    "    model.evaluate(loss='max_diff_loss')\n",
    "    \n",
    "    saved_plot_path = \"/kaggle/working/results/lstm_ae_plot\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".html\"\n",
    "    model.plot_results(size=1000, save_path=saved_plot_path)\n",
    "    \n",
    "    save_path='output_plots/test_predictions.png',\n",
    "    file_format='html'\n",
    "    saved_loss_plot_path = \"/kaggle/working/results/lstm_ae_loss_plot\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".png\"\n",
    "    model.plot_losses(save_path=saved_loss_plot_path)\n",
    "\n",
    "    # Save model and results\n",
    "    saved_model_path = \"/kaggle/working/results/lstm_ae_model\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".h5\"\n",
    "    results_csv = \"/kaggle/working/results/lstm_ae_results\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".csv\"\n",
    "    model.save_model(saved_model_path)\n",
    "\n",
    "    # Evaluate and Save Results to CSV\n",
    "    evaluate_model_and_save_results(\n",
    "        model_class=LSTMAutoencoder,\n",
    "        model_path=saved_model_path,\n",
    "        train_path=train_file,\n",
    "        test_path=test_file,\n",
    "        label_path=label_file,\n",
    "        results_csv_path=results_csv,\n",
    "        loss_type='max_diff'  # or 'mse'\n",
    "    )\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Zip the directory for downloading later\n",
    "!zip -r results.zip results"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM SAE Usage\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "run_list = [\"2\", \"37\", \"45\", \"23\", \"43\", \"21\", \"4\", \"8\", \"11\", \"20\", \"50\", \"51\", \"69\", \"31\", \"6\"]\n",
    "\n",
    "# Specify the directory you want to search in\n",
    "directory = \"/kaggle/working/anomaly_detection/UCR/UCR2_preprocessed/\"\n",
    "\n",
    "# Use glob to find all folders within the specified directory\n",
    "files = glob.glob(f\"{directory}/*/\")\n",
    "\n",
    "# Hyper-parameters\n",
    "HYPER_PARAMS = {\n",
    "    \"TIMESTEPS\": 128,\n",
    "    \"LATENT_DIM\": 32,\n",
    "    \"STEP_SIZE\": 10,\n",
    "    \"EPOCHS\": 200\n",
    "}\n",
    "\n",
    "# Make directory for results\n",
    "!rm -rf results\n",
    "!mkdir results\n",
    "\n",
    "# Gather all the time-series in the category\n",
    "train_files = sorted(glob.glob(os.path.join(directory, \"*_train.npy\")))\n",
    "\n",
    "# For each time-serie in a category train the model\n",
    "for ts in train_files:\n",
    "    # break # COMMENT THIS IF YOU WANT TO TRAIN LSTM\n",
    "    ts_id = ts.split(\"/\")[-1].split(\"_\")[0]\n",
    "    if not ts_id in run_list:\n",
    "            continue\n",
    "    print(f\"starting w/ ts {ts_id}\")\n",
    "    \n",
    "    # Find test and labels file for the time-serie\n",
    "    train_file = ts\n",
    "    test_file = ts.replace(\"_train.npy\", \"_test.npy\")\n",
    "    label_file = ts.replace(\"_train.npy\", \"_labels.npy\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train = np.load(train_file)\n",
    "    X_test = np.load(test_file)\n",
    "    Y_test = np.load(label_file)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = StationaryLSTMAutoencoder(train_data=X_train,\n",
    "                        test_data=X_test,\n",
    "                        labels=Y_test,\n",
    "                        timesteps=HYPER_PARAMS[\"TIMESTEPS\"],\n",
    "                        latent_dim=HYPER_PARAMS[\"LATENT_DIM\"],\n",
    "                        step_size=HYPER_PARAMS[\"STEP_SIZE\"])\n",
    "    \n",
    "    # Train\n",
    "    model.train(epochs=HYPER_PARAMS[\"EPOCHS\"], batch_size=32, patience=20) # Does not have loss='max_diff_loss', only MSE\n",
    "\n",
    "    # Evaluate\n",
    "    model.evaluate()\n",
    "    \n",
    "    saved_plot_path = \"/kaggle/working/results/lstm_sae_plot\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".html\"\n",
    "    model.plot_results(size=1000, save_path=saved_plot_path)\n",
    "    \n",
    "    file_format='html'\n",
    "    saved_loss_plot_path = \"/kaggle/working/results/lstm_sae_loss_plot\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".png\"\n",
    "    model.plot_losses(save_path=saved_loss_plot_path)\n",
    "\n",
    "    # Save model and results\n",
    "    saved_model_path = \"/kaggle/working/results/lstm_sae_model\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".h5\"\n",
    "    results_csv = \"/kaggle/working/results/lstm_sae_results\" + \"_ep\" + str(HYPER_PARAMS[\"EPOCHS\"]) + \"_ts\" + str(ts_id) + \".csv\"\n",
    "    model.save_model(saved_model_path)\n",
    "\n",
    "    # Evaluate and Save Results to CSV\n",
    "    evaluate_model_and_save_results(\n",
    "        model_class=StationaryLSTMAutoencoder,\n",
    "        model_path=saved_model_path,\n",
    "        train_path=train_file,\n",
    "        test_path=test_file,\n",
    "        label_path=label_file,\n",
    "        results_csv_path=results_csv,\n",
    "        loss_type='mse',  # or 'mse'\n",
    "        train_data=X_train,\n",
    "        test_data=X_test,\n",
    "        labels=Y_test,\n",
    "    )\n",
    "    # break\n"
   ]
  }
 ]
}
