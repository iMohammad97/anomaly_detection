{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyNvxU3vf4X/mw0SuqoiSqjy"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Packages",
   "metadata": {
    "id": "fB3U8aNVoWTs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install numpy tensorflow scikit-learn plotly pandas\n",
    "\n",
    "# For the UCR dataset we clone the git repo (if in Colab/Kaggle env)\n",
    "!git clone https://github.com/iMohammad97/anomaly_detection\n",
    "\n",
    "!pip install kaleido"
   ],
   "metadata": {
    "id": "jgIUvRdGoWI2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-12T13:23:15.830982Z",
     "iopub.execute_input": "2025-02-12T13:23:15.831286Z",
     "iopub.status.idle": "2025-02-12T13:23:34.646581Z",
     "shell.execute_reply.started": "2025-02-12T13:23:15.831264Z",
     "shell.execute_reply": "2025-02-12T13:23:34.645691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'anomaly_detection'...\nremote: Enumerating objects: 3828, done.\u001B[K\nremote: Counting objects: 100% (30/30), done.\u001B[K\nremote: Compressing objects: 100% (26/26), done.\u001B[K\nremote: Total 3828 (delta 16), reused 8 (delta 4), pack-reused 3798 (from 1)\u001B[K\nReceiving objects: 100% (3828/3828), 202.52 MiB | 33.48 MiB/s, done.\nResolving deltas: 100% (1623/1623), done.\nUpdating files: 100% (2721/2721), done.\nCollecting kaleido\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\nDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.9/79.9 MB\u001B[0m \u001B[31m21.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: kaleido\nSuccessfully installed kaleido-0.2.1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score, precision_score\n",
    "import glob, os, sys\n",
    "import kaleido\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import shutil\n",
    "import math"
   ],
   "metadata": {
    "id": "QlyJs93voTeo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739036175812,
     "user_tz": -210,
     "elapsed": 10718,
     "user": {
      "displayName": "Mohammad Mohammadi",
      "userId": "06409477078097051085"
     }
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-12T13:23:34.647871Z",
     "iopub.execute_input": "2025-02-12T13:23:34.648202Z",
     "iopub.status.idle": "2025-02-12T13:23:53.607582Z",
     "shell.execute_reply.started": "2025-02-12T13:23:34.648154Z",
     "shell.execute_reply": "2025-02-12T13:23:53.606669Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metrics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def pointwise_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Timepoint-wise precision: fraction of detected anomalies that are correct.\n",
    "    \"\"\"\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "def make_event(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Converts binary sequences (y_true and y_pred) into a list of (start, end) event tuples.\n",
    "    \"\"\"\n",
    "    y_true_starts = np.argwhere(np.diff(y_true.flatten(), prepend=0) == 1).flatten()\n",
    "    y_true_ends = np.argwhere(np.diff(y_true.flatten(), append=0) == -1).flatten()\n",
    "    y_true_events = list(zip(y_true_starts, y_true_ends))\n",
    "\n",
    "    y_pred_starts = np.argwhere(np.diff(y_pred, prepend=0) == 1).flatten()\n",
    "    y_pred_ends = np.argwhere(np.diff(y_pred, append=0) == -1).flatten()\n",
    "    y_pred_events = list(zip(y_pred_starts, y_pred_ends))\n",
    "\n",
    "    return y_true_events, y_pred_events\n",
    "\n",
    "\n",
    "def event_wise_recall(y_true_events, y_pred_events):\n",
    "    \"\"\"\n",
    "    Event-based recall. We consider an event 'detected' if the predicted event\n",
    "    overlaps with the true event in any way.\n",
    "    \"\"\"\n",
    "    detected_events = 0\n",
    "    for true_event in y_true_events:\n",
    "        true_start, true_end = true_event\n",
    "        for pred_event in y_pred_events:\n",
    "            pred_start, pred_end = pred_event\n",
    "            if pred_end >= true_start and pred_start <= true_end:\n",
    "                detected_events += 1\n",
    "                break\n",
    "    return detected_events / len(y_true_events) if y_true_events else 0\n",
    "\n",
    "\n",
    "def composite_f_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Combines timepoint precision and event-wise recall into a single F-score.\n",
    "    \"\"\"\n",
    "    prt = pointwise_precision(y_true, y_pred)\n",
    "    y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "    rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "    if prt + rece == 0:\n",
    "        return 0\n",
    "    return 2 * (prt * rece) / (prt + rece)\n",
    "\n",
    "\n",
    "def custom_auc_with_perfect_point(y_true, anomaly_scores, threshold_steps=100, plot=False):\n",
    "    \"\"\"\n",
    "    Generates thresholds, computes precision (timepoint) and recall (event-wise)\n",
    "    pairs, checks for a perfect point, and computes the AUC (area under the curve)\n",
    "    on the PR plane.\n",
    "    \"\"\"\n",
    "    percentiles = np.linspace(np.min(anomaly_scores), np.max(anomaly_scores) + 1e-7, threshold_steps)\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    perfect_point_found = False\n",
    "\n",
    "    for threshold in percentiles:\n",
    "        y_pred = (anomaly_scores >= threshold).astype(int)\n",
    "        prt = pointwise_precision(y_true, y_pred)\n",
    "\n",
    "        y_true_events, y_pred_events = make_event(y_true, y_pred)\n",
    "        rece = event_wise_recall(y_true_events, y_pred_events)\n",
    "\n",
    "        precision_list.append(prt)\n",
    "        recall_list.append(rece)\n",
    "\n",
    "        if prt == 1 and rece == 1:\n",
    "            perfect_point_found = True\n",
    "            break\n",
    "\n",
    "    # Compute AUC (precision vs recall)\n",
    "    custom_area = auc(recall_list, precision_list)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_list, precision_list, marker='o', label=f\"AUC = {custom_area:.4f}\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    return custom_area, perfect_point_found\n",
    "\n",
    "\n",
    "def compute_auc_pr(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-PR for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, anomaly_scores)\n",
    "        auc_pr = auc(recall, precision)\n",
    "    except ValueError:\n",
    "        print(\"AUC-PR computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_pr = np.nan\n",
    "    return auc_pr\n",
    "\n",
    "\n",
    "def compute_auc_roc(y_true, anomaly_scores):\n",
    "    \"\"\"\n",
    "    Compute AUC-ROC for time-series anomaly detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(y_true, anomaly_scores)\n",
    "    except ValueError:\n",
    "        print(\"AUC-ROC computation failed: Ensure both classes (0 and 1) are present in y_true.\")\n",
    "        auc_roc = np.nan\n",
    "    return auc_roc"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_windows(data, window_size: int, step_size: int = 1):\n",
    "    \"\"\"\n",
    "    Given a 2D array `data` of shape (N, features), create overlapping windows\n",
    "    of shape (window_size, features). Returns array of shape (M, window_size, features).\n",
    "    If data is shorter than window_size, returns None.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    if N < window_size:\n",
    "        return None\n",
    "    windows = []\n",
    "    for i in range(0, N - window_size + 1, step_size):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return np.stack(windows, axis=0)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "class UCR(Dataset):\n",
    "    def __init__(self, path: str, window_size: int, step_size: int = 1, train: bool = True, data_id: int = 0):\n",
    "        self.path = path\n",
    "        self.window_size, self.step_size = window_size, step_size\n",
    "        if train:\n",
    "            self.data = self.create_windows('train', data_id)\n",
    "            self.labels = torch.zeros_like(self.data)\n",
    "        else:\n",
    "            self.data = self.create_windows('test', data_id)\n",
    "            self.labels = self.create_windows('labels', data_id)\n",
    "\n",
    "    def create_windows(self, tag: str, data_id: int):\n",
    "        files = [f for f in os.listdir(self.path) if f.endswith(f'{tag}.npy')]\n",
    "        windows = []\n",
    "        for file_path in files:\n",
    "            if data_id != 0 and not file_path.startswith(f'{str(data_id)}_'):\n",
    "                continue\n",
    "            array = np.load(os.path.join(self.path, file_path))\n",
    "            for i in range(0, len(array) - self.window_size + 1, self.step_size):\n",
    "                windows.append(array[i:i + self.window_size])\n",
    "        windows = np.array(windows) # because it's faster\n",
    "        return torch.tensor(windows, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "        return self.data[idx]\n",
    "\n",
    "def get_dataloaders(path: str, window_size: int, batch_size: int, step_size: int = 1, data_id: int = 0, shuffle: bool = False, seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    # Create datasets\n",
    "    train_dataset = UCR(path, window_size, step_size=step_size, train=True, data_id=data_id)\n",
    "    test_dataset = UCR(path, window_size, step_size=1, train=False, data_id=data_id) # test step size should always be 1\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :].to(x.device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM AutoEncoder"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMAutoencoder:\n",
    "    def __init__(self,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 labels,\n",
    "                 timesteps: int = 128,\n",
    "                 features: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 lstm_units: int = 64,\n",
    "                 step_size: int = 1,\n",
    "                 threshold_sigma=2.0,\n",
    "                 seed: int = 0):\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.labels = labels\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.step_size = step_size\n",
    "        self.threshold_sigma = threshold_sigma\n",
    "\n",
    "        # Prepare windowed data\n",
    "        self.train_data_window = create_windows(self.train_data, timesteps, step_size)\n",
    "        self.test_data_window = create_windows(self.test_data, timesteps, 1)\n",
    "\n",
    "        # Model placeholders\n",
    "        self.model = None\n",
    "        self.threshold = 0\n",
    "\n",
    "        # Arrays to hold predictions\n",
    "        if self.test_data_window is not None:\n",
    "            self.predictions_windows = np.zeros(len(self.test_data_window))\n",
    "        self.anomaly_preds = np.zeros(len(self.test_data))\n",
    "        self.anomaly_errors = np.zeros(len(self.test_data))\n",
    "        self.predictions = np.zeros(len(self.test_data))\n",
    "\n",
    "        self.losses = {'train': [], 'valid': []}\n",
    "        self.name = 'LSTMAutoencoder'  # A name attribute for the class\n",
    "\n",
    "        set_seed(seed)\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = tf.keras.Input(shape=(self.timesteps, self.features), name='input_layer')\n",
    "\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True, name='lstm_1')(inputs)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=False, name='latent')(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.timesteps, name='repeat_vector')(x)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=True, name='lstm_3')(x)\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True, name='lstm_4')(x)\n",
    "        outputs = layers.TimeDistributed(layers.Dense(self.features, name='dense_output'))(x)\n",
    "\n",
    "        self.model = models.Model(inputs, outputs, name='model')\n",
    "\n",
    "    def compute_threshold(self):\n",
    "        rec = self.model.predict(self.train_data_window, verbose=0)\n",
    "        mse = np.mean(np.square(self.train_data_window - rec), axis=(1, 2))\n",
    "        self.threshold = np.mean(mse) + self.threshold_sigma * np.std(mse)\n",
    "\n",
    "    def train(self,\n",
    "              batch_size=32,\n",
    "              epochs=50,\n",
    "              optimizer='adam',\n",
    "              loss='mse',\n",
    "              patience=10,\n",
    "              shuffle: bool = False,\n",
    "              seed: int = 42):\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Custom max-diff loss function\n",
    "        def max_diff_loss(y_true, y_pred):\n",
    "            return tf.reduce_max(tf.abs(y_true - y_pred), axis=[1, 2])\n",
    "\n",
    "        # Determine which loss function to use\n",
    "        loss_function = 'mse' if loss == 'mse' else max_diff_loss\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            self.train_data_window, self.train_data_window,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            validation_split=0.1,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        self.losses['train'] = [float(l) for l in history.history['loss']]\n",
    "        self.losses['valid'] = [float(l) for l in history.history['val_loss']]\n",
    "\n",
    "    def evaluate(self, batch_size=32, loss='mse'):\n",
    "        \"\"\"\n",
    "        Evaluate the model on self.test_data_window.\n",
    "        Sets self.anomaly_preds, self.anomaly_errors, self.predictions.\n",
    "        \"\"\"\n",
    "        if self.test_data_window is None or len(self.test_data_window) == 0:\n",
    "            print(\"No test windows available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        length = self.test_data.shape[0]\n",
    "        self.compute_threshold()\n",
    "\n",
    "        # Generate predictions for the test data windows\n",
    "        self.predictions_windows = self.model.predict(self.test_data_window, batch_size=batch_size)\n",
    "\n",
    "        # Compute reconstruction errors\n",
    "        if loss == 'mse':\n",
    "            errors = np.mean(np.square(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "        else:\n",
    "            # If using max_diff_loss\n",
    "            errors = np.max(np.abs(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "\n",
    "        # Expand window errors to match original time steps\n",
    "        M = errors.shape[0]\n",
    "        timestep_errors = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            timestep_errors[start:end + 1] += errors[i]\n",
    "            counts[start:end + 1] += 1\n",
    "\n",
    "        counts[counts == 0] = 1  # Avoid division by zero\n",
    "        timestep_errors /= counts  # Average overlapping windows\n",
    "\n",
    "        self.anomaly_preds = (timestep_errors > self.threshold).astype(int)\n",
    "        self.anomaly_errors = timestep_errors\n",
    "\n",
    "        # Compute predictions (averaged across windows)\n",
    "        counts = np.zeros(length)\n",
    "        self.predictions = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                timestep_index = i + j\n",
    "                if timestep_index < length:\n",
    "                    self.predictions[timestep_index] += self.predictions_windows[i, j]\n",
    "                    counts[timestep_index] += 1\n",
    "\n",
    "        # Avoid division by zero\n",
    "        for i in range(length):\n",
    "            if counts[i] > 0:\n",
    "                self.predictions[i] /= counts[i]\n",
    "\n",
    "        self.predictions = np.nan_to_num(self.predictions)\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        \"\"\"\n",
    "        Returns latent representation from the encoder part of the model.\n",
    "        \"\"\"\n",
    "        encoder_model = models.Model(inputs=self.model.input,\n",
    "                                    outputs=self.model.get_layer('latent').output)\n",
    "        latent_representations = encoder_model.predict(x)\n",
    "        return latent_representations\n",
    "\n",
    "    def save_model(self, model_path: str = \"model.h5\"):\n",
    "        \"\"\"\n",
    "        Save the Keras model to disk.\n",
    "        \"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            print(\"No model to save.\")\n",
    "\n",
    "    def load_model(self, model_path: str, train_path: str, test_path: str, label_path: str):\n",
    "        \"\"\"\n",
    "        Load the Keras model from the specified file paths and set\n",
    "        self.train_data, self.test_data, and self.labels accordingly.\n",
    "        \"\"\"\n",
    "        self.model = models.load_model(model_path, compile=False)\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "        # Load data\n",
    "        self.train_data = np.load(train_path)\n",
    "        self.test_data = np.load(test_path)\n",
    "        self.labels = np.load(label_path)\n",
    "\n",
    "        # Recreate windows\n",
    "        self.train_data_window = create_windows(self.train_data, self.timesteps)\n",
    "        self.test_data_window = create_windows(self.test_data, self.timesteps)\n",
    "\n",
    "        print(f\"Loaded model from {model_path} and data from {train_path}, {test_path}, {label_path}.\")\n",
    "\n",
    "    def plot_results(self, save_path=None, file_format='html',size=800):\n",
    "        \"\"\"\n",
    "        Plot test data, predictions, anomaly errors, and highlight\n",
    "        labeled anomalies and predicted anomalies.\n",
    "        \"\"\"\n",
    "        # Flatten arrays\n",
    "        test_data = self.test_data.ravel()\n",
    "        anomaly_preds = self.anomaly_preds\n",
    "        anomaly_errors = self.anomaly_errors\n",
    "        predictions = self.predictions\n",
    "        labels = self.labels.ravel()\n",
    "\n",
    "        if not (len(test_data) == len(labels) == len(anomaly_preds) == len(anomaly_errors) == len(predictions)):\n",
    "            raise ValueError(\"All input arrays must have the same length.\")\n",
    "\n",
    "        plot_width = max(size, len(test_data) // 10)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        # Test Data\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data',\n",
    "                                 line=dict(color='blue')))\n",
    "        # Predictions\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(predictions))),\n",
    "                                 y=predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Predictions',\n",
    "                                 line=dict(color='purple')))\n",
    "        # Anomaly Errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(anomaly_errors))),\n",
    "                                 y=anomaly_errors,\n",
    "                                 mode='lines',\n",
    "                                 name='Anomaly Errors',\n",
    "                                 line=dict(color='red')))\n",
    "\n",
    "        # Labeled anomalies\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels on Test Data',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Predicted anomalies\n",
    "        anomaly_pred_indices = [i for i in range(len(anomaly_preds)) if anomaly_preds[i] == 1]\n",
    "        if anomaly_pred_indices:\n",
    "            fig.add_trace(go.Scatter(x=anomaly_pred_indices,\n",
    "                                     y=[predictions[i] for i in anomaly_pred_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Anomaly Predictions',\n",
    "                                     marker=dict(color='green', size=10)))\n",
    "\n",
    "        fig.update_layout(title='Test Data, Predictions, and Anomalies',\n",
    "                          xaxis_title='Time Steps',\n",
    "                          yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    "                          template='plotly',\n",
    "                          width=plot_width)\n",
    "\n",
    "        # Optionally save the figure\n",
    "        if save_path is not None:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "            if file_format.lower() == 'html':\n",
    "                # Save as interactive HTML\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                # Save as static image (requires kaleido or orca)\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    def plot_losses(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot training and validation losses.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses['train'], label='Training Loss')\n",
    "        plt.plot(self.losses['valid'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "            # Save the figure\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM SAE"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "class StationaryLoss(layers.Layer):\n",
    "    def call(self, latent, mean_coef: float = 1.0, std_coef: float = 1.0):\n",
    "        # Calculate the average of the latent space\n",
    "        latent_avg = tf.reduce_mean(latent, axis=0)\n",
    "        mse_loss = tf.reduce_mean(tf.abs(latent_avg))\n",
    "        self.add_loss(mean_coef * mse_loss)\n",
    "        \n",
    "        # Calculate the standard deviation of the latent space\n",
    "        latent_std = tf.math.reduce_std(latent, axis=0)\n",
    "        std_loss = tf.reduce_mean(tf.abs(latent_std - 1.0))\n",
    "        self.add_loss(std_coef * std_loss)\n",
    "        \n",
    "        # Store the losses separately for logging\n",
    "        self.mse_loss = mean_coef * mse_loss\n",
    "        self.std_loss = std_coef * std_loss\n",
    "        \n",
    "        return latent\n",
    "\n",
    "\n",
    "class StationaryLSTMAutoencoder:\n",
    "    def __init__(self, train_data, test_data, labels, timesteps: int = 128, features: int = 1, latent_dim: int = 32, lstm_units: int = 64, step_size: int = 1, threshold_sigma=2.0, seed: int = 0):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.train_data_window = create_windows(self.train_data, timesteps, step_size)\n",
    "        self.test_data_window = create_windows(self.test_data, timesteps, 1)\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = None  # Model is not built yet.\n",
    "        self.threshold_sigma = threshold_sigma\n",
    "        self.threshold = 0\n",
    "        self.predictions_windows = np.zeros(len(self.test_data_window))\n",
    "        self.anomaly_preds = np.zeros(len(self.test_data))\n",
    "        self.anomaly_errors = np.zeros(len(self.test_data))\n",
    "        self.predictions = np.zeros(len(self.test_data))\n",
    "        self.labels = labels\n",
    "        self.name = \"LSTM_SAE\"\n",
    "        self.losses = {\"mse\": [], \"mean\": [], \"std\": []}\n",
    "        set_seed(seed)\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Encoder\n",
    "        inputs = tf.keras.Input(shape=(self.timesteps, self.features))\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True)(inputs)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=False)(x)\n",
    "        latent = layers.Dense(self.latent_dim)(x)\n",
    "\n",
    "        # Apply custom loss to the latent space\n",
    "        latent_with_loss = StationaryLoss()(latent, mean_coef=1.0, std_coef=1.0)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.timesteps)(latent_with_loss)\n",
    "        x = layers.LSTM(self.latent_dim, return_sequences=True)(x)\n",
    "        x = layers.LSTM(self.lstm_units, return_sequences=True)(x)\n",
    "        outputs = layers.TimeDistributed(layers.Dense(self.features))(x)\n",
    "\n",
    "        # DAE Model\n",
    "        self.model = models.Model(inputs, outputs)  # Return only the outputs (no KL divergence in this case)\n",
    "\n",
    "    def train(self, batch_size: int = 32, epochs: int = 50, optimizer: str = 'adam', patience: int = 15, seed: int = 42, shuffle: bool = False):\n",
    "        set_seed(seed)\n",
    "        # Ensure the optimizer is set up correctly\n",
    "        if isinstance(optimizer, str):\n",
    "            optimizer = tf.keras.optimizers.get(optimizer)  # Get optimizer by name\n",
    "        elif not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n",
    "            raise ValueError(\"Optimizer must be a string or a tf.keras.optimizers.Optimizer instance.\")\n",
    "\n",
    "        # Loss function\n",
    "        mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Track losses\n",
    "        mse_loss_tracker = tf.keras.metrics.Mean(name=\"mse_loss\")\n",
    "        mean_loss_tracker = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "        std_loss_tracker = tf.keras.metrics.Mean(name=\"std_loss\")\n",
    "\n",
    "        # Early stopping variables \n",
    "        best_epoch_loss = float('inf') \n",
    "        patience_counter = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            mse_loss_tracker.reset_state()\n",
    "            mean_loss_tracker.reset_state()\n",
    "            std_loss_tracker.reset_state()\n",
    "\n",
    "            if shuffle:\n",
    "                np.random.shuffle(self.train_data_window)\n",
    "\n",
    "            for step in trange(0, len(self.train_data_window), batch_size, leave=False):\n",
    "                batch_data = self.train_data_window[step:step + batch_size]\n",
    "                epoch_loss = 0 # Should be changed to val loss later\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    reconstructed = self.model(batch_data, training=True)\n",
    "\n",
    "                    # Compute reconstruction loss\n",
    "                    mse_loss = mse_loss_fn(batch_data, reconstructed)\n",
    "\n",
    "                    # Get custom losses from the model\n",
    "                    mean_loss = tf.reduce_mean([layer.mse_loss for layer in self.model.layers if isinstance(layer, StationaryLoss)])\n",
    "                    std_loss = tf.reduce_mean([layer.std_loss for layer in self.model.layers if isinstance(layer, StationaryLoss)])\n",
    "\n",
    "                    # Total loss\n",
    "                    total_loss = mse_loss + mean_loss + std_loss\n",
    "\n",
    "                # Compute gradients and update weights\n",
    "                gradients = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "\n",
    "                # Track losses\n",
    "                mse_loss_tracker.update_state(mse_loss)\n",
    "                mean_loss_tracker.update_state(mean_loss)\n",
    "                std_loss_tracker.update_state(std_loss)\n",
    "\n",
    "                epoch_loss += total_loss\n",
    "\n",
    "            # Log losses after each epoch\n",
    "            self.losses['mse'].append(float(mse_loss_tracker.result().numpy()))\n",
    "            self.losses['mean'].append(float(mean_loss_tracker.result().numpy()))\n",
    "            self.losses['std'].append(float(std_loss_tracker.result().numpy()))\n",
    "            pbar.set_description(\n",
    "                f\"MSE Loss = {self.losses['mse'][-1]:.4f}, Mean Loss = {self.losses['mean'][-1]:.4f}, STD Loss = {self.losses['std'][-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping logic \n",
    "            if epoch_loss < best_epoch_loss: \n",
    "                best_epoch_loss = epoch_loss \n",
    "                patience_counter = 0  \n",
    "            else: \n",
    "                patience_counter += 1 \n",
    "                if patience_counter >= patience: \n",
    "                    print(f\"Early stopping triggered after {epoch + 1} epochs.\") \n",
    "                    break\n",
    "\n",
    "    def compute_threshold(self):\n",
    "        rec = self.model.predict(self.train_data_window, verbose=0)\n",
    "        mse = np.mean(np.square(self.train_data_window - rec), axis=(1, 2))\n",
    "        self.threshold = np.mean(mse) + self.threshold_sigma * np.std(mse)\n",
    "\n",
    "    def evaluate(self, batch_size=32):\n",
    "        length = self.test_data.shape[0]\n",
    "        self.compute_threshold()\n",
    "        # Generate predictions for the test data windows\n",
    "        self.predictions_windows = self.model.predict(self.test_data_window, batch_size=batch_size)\n",
    "        mse = np.mean(np.square(self.test_data_window - self.predictions_windows), axis=(1, 2))\n",
    "\n",
    "        # Expand errors to original length\n",
    "        M = mse.shape[0]\n",
    "        timestep_errors = np.zeros(length)\n",
    "        counts = np.zeros(length)\n",
    "\n",
    "        # Each window i covers timesteps [i, i+window_size-1]\n",
    "        for i in range(M):\n",
    "            start = i\n",
    "            end = i + self.timesteps - 1\n",
    "            timestep_errors[start:end + 1] += mse[i]\n",
    "            counts[start:end + 1] += 1\n",
    "\n",
    "        counts[counts == 0] = 1  # Avoid division by zero\n",
    "        timestep_errors /= counts  # Average overlapping windows\n",
    "\n",
    "        # Generate anomaly predictions based on the threshold\n",
    "        self.anomaly_preds = (timestep_errors > self.threshold).astype(int)\n",
    "        self.anomaly_errors = timestep_errors\n",
    "\n",
    "        counts = np.zeros(length)\n",
    "        for i in range(M):\n",
    "            for j in range(self.timesteps):\n",
    "                timestep_index = i + j  # This is the index in the timestep corresponding to the current prediction\n",
    "                if timestep_index < length:  # Ensure we don't go out of bounds\n",
    "                    self.predictions[timestep_index] += self.predictions_windows[i, j]  # Accumulate each prediction appropriately\n",
    "                    counts[timestep_index] += 1\n",
    "\n",
    "        # Divide by counts to get the average prediction\n",
    "        for i in range(length):\n",
    "            if counts[i] > 0:\n",
    "                self.predictions[i] /= counts[i]\n",
    "\n",
    "        self.predictions = np.nan_to_num(self.predictions)\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        encoder_model = models.Model(inputs=self.model.input, outputs=self.model.get_layer('latent').output)\n",
    "        latent_representations = encoder_model.predict(x)\n",
    "        return latent_representations\n",
    "\n",
    "\n",
    "    def save_model(self, model_path: str = \"model.h5\"):\n",
    "        \n",
    "        # Save the Keras model\n",
    "        if self.model is not None:\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            print(\"No model to save.\")\n",
    "\n",
    "    \n",
    "    def load_model(self, model_path: str, train_path: str, test_path: str, label_path: str):\n",
    "        # Use custom_object_scope for the custom layer\n",
    "        with custom_object_scope({'StationaryLoss': StationaryLoss}):\n",
    "            self.model = models.load_model(\n",
    "                model_path,\n",
    "                compile=False  # Avoid recompiling until the model is fully loaded\n",
    "            )\n",
    "    \n",
    "        # Compile the model for evaluation or retraining\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error']\n",
    "        )\n",
    "    \n",
    "        # Load data\n",
    "        self.train_data = np.load(train_path)\n",
    "        self.test_data = np.load(test_path)\n",
    "        self.labels = np.load(label_path)\n",
    "    \n",
    "        # Recreate the windows with the newly loaded data\n",
    "        self.train_data_window = create_windows(self.train_data, self.timesteps)\n",
    "        self.test_data_window = create_windows(self.test_data, self.timesteps)\n",
    "    \n",
    "    def plot_results(self, size=800, save_path=None, file_format='html'):\n",
    "        # Flattening arrays to ensure they are 1D\n",
    "        test_data = self.test_data.ravel()  # Convert to 1D array\n",
    "        anomaly_preds = self.anomaly_preds  # Already 1D\n",
    "        anomaly_errors = self.anomaly_errors  # Already 1D\n",
    "        predictions = self.predictions  # Already 1D\n",
    "        labels = self.labels.ravel()  # Convert to 1D array\n",
    "\n",
    "        plot_width = max(size, len(test_data) // 10)  # Ensure a minimum width of 800, scale with data length\n",
    "\n",
    "        # Check if all inputs have the same length\n",
    "        if not (len(test_data) == len(labels) == len(anomaly_preds) == len(anomaly_errors) == len(predictions)):\n",
    "            raise ValueError(\"All input arrays must have the same length.\")\n",
    "\n",
    "        # Create a figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add traces for test data, predictions, and anomaly errors\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(test_data))),\n",
    "                                 y=test_data,\n",
    "                                 mode='lines',\n",
    "                                 name='Test Data'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(predictions))),\n",
    "                                 y=predictions,\n",
    "                                 mode='lines',\n",
    "                                 name='Predictions'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(anomaly_errors))),\n",
    "                                 y=anomaly_errors,\n",
    "                                 mode='lines',\n",
    "                                 name='Anomaly Errors'))\n",
    "\n",
    "        # Highlight points in test_data where label is 1\n",
    "        label_indices = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "        if label_indices:\n",
    "            fig.add_trace(go.Scatter(x=label_indices,\n",
    "                                     y=[test_data[i] for i in label_indices],\n",
    "                                     mode='markers',\n",
    "                                     name='Labels on Test Data',\n",
    "                                     marker=dict(color='orange', size=10)))\n",
    "\n",
    "        # Set the layout\n",
    "        fig.update_layout(title='Test Data, Predictions, and Anomalies',\n",
    "                          xaxis_title='Time Steps', yaxis_title='Value',\n",
    "                          legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    "                          template='plotly', width=plot_width)\n",
    "        # Optionally save the figure\n",
    "        if save_path is not None:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "            if file_format.lower() == 'html':\n",
    "                # Save as interactive HTML\n",
    "                fig.write_html(save_path)\n",
    "            else:\n",
    "                # Save as static image (requires kaleido or orca)\n",
    "                fig.write_image(save_path, format=file_format)\n",
    "\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "    def plot_losses(self, save_path=None):\n",
    "        # Plot the loss values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses['mse'], label='MSE Reconstruction Loss')\n",
    "        plt.plot(self.losses['mean'], label='Latent Mean Loss')\n",
    "        plt.plot(self.losses['std'], label='Latent Standard Deviation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if save_path:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "            # Save the figure\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  }
 ]
}
